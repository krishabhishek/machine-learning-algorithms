\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  citecolor=blue
}


\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

\setlength\titlebox{5cm}

\title{A Survey of Neural Network Techniques for Feature Extraction from Text}

\author{
  Vineet John \\
  University of Waterloo \\
  {\tt v2john@uwaterloo.ca} \\
}

\date{}

\begin{document}

\maketitle


\begin{abstract}
  This paper aims to catalyze the discussions about text feature extraction techniques using neural network techniques. 
\end{abstract}


\section{Introduction} % (fold)
\label{sec:introduction}

  A majority of the methods currently in use for text-based feature extraction rely on relatively simple statistical techniques. For instance, a word co-occurrence model like N-Grams or a bag-of-words model like TF-IDF.

  The motivation of this research project is to identify and survey the techniques that use neural networks and to compare them to the traditional text feature extraction models.

  Feature extraction of text can be used for a multitude of applications including - but not limited to - unsupervised semantic similarity detection, article classification and sentiment analysis.

% section introduction (end)

\section{Research Questions} % (fold)
\label{sec:research_questions}

  \begin{itemize}
    \item [RQ1] What are the simpler statistical techniques to extract features from text?
    \item [RQ2] Is there any inherent benefit to using neural networks as opposed to the simple methods?
    \item [RQ3] What are the trade-offs that neural networks incur as opposed to the simple methods?
    \item [RQ4] How do the different techniques compare to each other in terms of performance and accuracy?
    \item [RQ5] In what use-cases do the trade-offs outweigh the benefits of neural networks?
  \end{itemize}

% section research_questions (end)

\section{Methodology} % (fold)
\label{sec:methodology}

  The research questions listed in Section~\ref{sec:research_questions} will be tackled by surveying a few of the important overview papers on the topic\cite{goldberg2016primer}\cite{bengio2003neural}\cite{morin2005hierarchical}. A few of the groundbreaking research papers in this area will also be studied, including Word2Vec\cite{mikolov2013efficient}\cite{mikolov2013distributed}\cite{mikolov2013linguistic}.

  In addition to this, other application areas in NLP will be surveyed, including tasks like part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. \cite{socher2011parsing}\cite{luong2013better}\cite{maas2015lexicon}\cite{li2015hierarchical}\cite{collobert2011natural}\cite{pennington2014glove}

% section methodology (end)

\section{Goal} % (fold)
\label{sec:goal}

  The goal of this project is the documentation of the differences, advantages and drawbacks in the domain of feature extraction from text data using neural networks. 

  The project report could serve as a quick cheat-sheet for engineers looking to build a text classification or scoring pipeline, as the comparison would serve to map a use-case to a certain type of feature extraction approach.

% section goal (end)


\section{Document Vectorization} % (fold)
\label{sec:document_vectorization}

  Document vectorization is needed to convert the text content of the SemEval headlines into a numeric vector representation that can be utilized as feature vectors, which can then be used to train a machine learning model on. For the purposes of this project, the methods for vectorization considered are listed in the subsections below. \cite{SemEvalPaper}

  \subsection{N-gram Model} % (fold)
  \label{sub:n_gram_model}
    N-grams are contiguous sequences of `n' items from a given sequence of text or speech. Given a complete corpus of documents, each tuple of `n' grams, either characters or words are represented by a unique bit in a bit vector, which when aggregated for a body of text, form a sparse vectorized representation of the text in the form of n-gram occurrences.
  
  % subsubsection n_gram_model (end)

  \subsection{TF-IDF Model} % (fold)
  \label{sub:tf_idf_model}
    Term frequency-inverse document frequency (TF-IDF), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus \cite{sparck1972statistical}. The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. It is a bag-of-words model, and doesn't preserve word ordering.
  
  % subsubsection tf_idf_model (end)

  \subsection{Paragraph Vector Model} % (fold)
  \label{sub:paragraph_vectors_doc2vec}

    A Paragraph Vector representation of text is an unsupervised learning algorithm that learns fixed-size vector representations for variable-length pieces of texts such as sentences and documents \cite{le2014distributed}. The vector representations are learned to predict the surrounding words in contexts sampled from the paragraph. In the context of the SemEval headlines, the vector representations were learned for the complete headline.

    Two distinct implementations were explored while attempting to vectorize the headlines using the Paragraph Vector approach.
    \begin{itemize}
      \item 
        Doc2Vec: A Python library implementation in Gensim\footnote{https://radimrehurek.com/gensim/models/doc2vec.html}.
      \item 
        FastText: A standalone implementation in C++ \cite{bojanowski2016enriching} \cite{joulin2016bag}.
    \end{itemize}
    Doc2Vec was the final choice that was opted for due to the ease of integration into the existing system.
  
  % subsection paragraph_vectors_doc2vec (end)

% section document_vectorization (end)



\section{A Primer of Neural Net Models for NLP} % (fold)
\label{sec:a_primer_of_neural_net_models_for_nlp}

  \begin{itemize}
    \item 
    Fully connected feed-forward neural networks are non-linear learners that can, be used as a drop-in replacement wherever a linear learner is used.
    \item 
    The high accuracy is a result of this non-linearity along with the availability of pre-trained word embeddings.
    \item 
    Multi-layer feed-forward networks can provide competitive results on sentiment classification and factoid question answering
    \item 
    Convolutional and pooling architecture show promising results on many tasks, including document classification, short-text categorization, sentiment classification, relation type classification between entities, event detection, paraphrase identification, semantic role labeling, question answering, predicting box-office revenues of movies based on critic reviews, modeling text interestingness, and modeling the relation between character-sequences and part-of-speech tags.
    \item 
    Convolutional and pooling architectures allow us to encode arbitrary large items as fixed size vectors capturing their most salient features, they do so by sacrificing most of the structural information.
    \item 
    Recurrent and recursive networks allows using sequences and trees and preserve the structural information.
    \item 
    Recurrent models have been shown to produce very strong results for language modeling as well as for sequence tagging, machine translation, dependency parsing, sentiment analysis, noisy text normalization, dialog state tracking, response generation, and modeling the relation between character sequences and part-of-speech tags. 
    \item 
    Recursive models were shown to produce state-of-the-art or near state-of-the-art results for constituency and dependency parse re-ranking, discourse parsing, semantic relation classification, political ideology detection based on parse trees, sentiment classification, target-dependent sentiment classification and question answering.
    \item 
    Convolutional nets seem to work well for summarization related tasks and recurrent/recursive nets seem to work well for language modeling tasks.
  \end{itemize}

% section a_primer_of_neural_net_models_for_nlp (end)


\section{A Neural Probabilistic Language Model} % (fold)
\label{sec:a_neural_probabilistic_language_model}

  \textbf{Background:}
  \begin{itemize}
    \item 
    We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences.
    \item 
    A fundamental problem that makes language modeling and other learning problems difficult is the curse of dimensionality. It is particularly obvious in the case when one wants to model the joint distribution between many discrete random variables (such as words in a sentence, or discrete attributes in a data-mining task)
    \item 
    State-of-the art results are typically obtained using trigrams.
    \item 
    Language generation via subtitution of semantically similar language constructs of existing sentences can be done via shared-parameter multi-layer neural networks.
    \item 
    The objective of this paper is to obtain real-valued vector sequences of words and learn a joint probability function for those sequences of words alongside the feature vector, and hence, jointly learn both the real-value vector representation and the parameters of the probability distribution.
    \item 
    This probability function can be tuned in order to maximize log-likelihood of the training data, while penalizing with a cost function similar to the one used in Ridge regression.
    \item 
    This will ensure that semantically similar words end up with an almost equivalent feature vectors, called learned distributed feature vectors.
    \item 
    A challenge with modeling discrete variables like a sentence structure as opposed to a continuous value is that the continuous valued function can be assumed to have some form of locality, but the same assumption cannot be made in case of discrete functions.
    \item 
    N-gram models try to acheive a statistical modeling of languages by calculating the conditional probabilities of each possible word that can follow a set of $n$ preceding words.
    \item 
    New sequences of words can be generated by effectively gluing together the popular combinations i.e. n-grams with very high frequency counts.
  \end{itemize}

  \textbf{Goal of the paper:}
  Knowing the basic structure of a sentence, we should be able to create a new sentence by replacing parts of the old sentence with interchangeable elements.

  \textbf{Challenges:}
  \begin{itemize}
    \item 
    The main bottleneck for the neural computation is while computing the activations of the output layer
  \end{itemize}

  \textbf{Optimizations:}
  \begin{itemize}
    \item 
    Data parallel processing (different processor working on a different subsets of data) and asynchronous processor usage of shared memory.
  \end{itemize}

% section a_neural_probabilistic_language_model (end)


\section{A Hierarchical Neural Autoencoder for Paragraphs and Documents} % (fold)
\label{sec:a_hierarchical_neural_autoencoder_for_paragraphs_and_documents}

  \begin{itemize}
    \item 
    Attempts to build a paragraph embedding from the underlying word and sentence embeddings, and then proceeds to encode the paragraph embedding in an attempt to reconstruct the original paragraph.
    \item 
    For this to happen, we need to preserve, syntactic, semantic and discourse related properties while creating the embedded representation.
    \item 
    Hierarchical LSTM utilized to preserve sentence structure.
  \end{itemize}

  \textbf{Implementation:}
  \begin{itemize}
    \item 
    An LSTM layer to convert words into a vector representation of a sentence. Another LSTM layer after that to convert multiple sentences into a paragraph.
    \item 
    Parameters are estimated by maximizing likelihood of outputs given inputs, similar to standard sequence-to-sequence models.
    \item 
    Estimations are calculated using softmax functions to maximize the likelihood of the consituent words.
    \item 
    Attention models using the hierarchical autoencoder could be utilized for dialogue systems, since it explicitly models for discourse.
  \end{itemize}

% section a_hierarchical_neural_autoencoder_for_paragraphs_and_documents (end)


\section{Hierarchical Probabilistic Neural Network Language Model} % (fold)
\label{sec:hierarchical_probabilistic_neural_network_language_model}

  \textbf{Goal}
  Implementing a  hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy

  \textbf{Summary:}
  \begin{itemize}
    \item 
    Similar to the previous paper, attempts to tackle the `curse of dimensionality' as in \ref{sec:a_neural_probabilistic_language_model}. 
    \item 
    It attempts to produce a much faster variant.
    \item 
    Back-off n-grams are used for this approach too, and we attempt to learn a real-valued vector representation of each word.
    \item 
    The word embeddings learnt are shared across all the participating nodes in the distributed architecture.
  \end{itemize}

% section hierarchical_probabilistic_neural_network_language_model (end)


\newpage

\bibliographystyle{unsrt}
\bibliography{cs698_project_report}

\end{document}
