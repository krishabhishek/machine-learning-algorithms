\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  citecolor=blue
}

\aclfinalcopy 

\setlength\titlebox{5cm}

\title{A Survey of Neural Network Techniques for Feature Extraction from Text}

\author{
  Vineet John \\
  University of Waterloo \\
  {\tt v2john@uwaterloo.ca} \\
}

\date{}

\begin{document}

\maketitle


\begin{abstract}
  This paper aims to catalyze the discussions about text feature extraction techniques using neural network techniques, for the CS 698 Final Project. The research questions focus on the newly emerging areas of neural network techniques that have proven to be useful tools for language processing, language generation, text classification and other computational linguistics areas.
\end{abstract}


\section{Introduction} % (fold)
\label{sec:introduction}

  A majority of the methods currently in use for text-based feature extraction rely on relatively simple statistical techniques. For instance, a word co-occurrence model like N-Grams or a bag-of-words model like TF-IDF.

  The motivation of this research project is to identify and survey the techniques that use neural networks and to compare them to the traditional text feature extraction models.

  Feature extraction of text can be used for a multitude of applications including - but not limited to - unsupervised semantic similarity detection, article classification and sentiment analysis.

% section introduction (end)

\section{Research Questions} % (fold)
\label{sec:research_questions}

  \begin{itemize}
    \item [RQ1] What are the simpler statistical techniques to extract features from text?
    \item [RQ2] Is there any inherent benefit to using neural networks as opposed to the simple methods?
    \item [RQ3] What are the trade-offs that neural networks incur as opposed to the simple methods?
    \item [RQ4] How do the different techniques compare to each other in terms of performance and accuracy?
    \item [RQ5] In what use-cases do the trade-offs outweigh the benefits of neural networks?
  \end{itemize}

% section research_questions (end)

\section{Methodology} % (fold)
\label{sec:methodology}

  The research questions listed in Section~\ref{sec:research_questions} will be tackled by surveying a few of the important overview papers on the topic\cite{goldberg2016primer}\cite{bengio2003neural}\cite{morin2005hierarchical}. A few of the groundbreaking research papers in this area will also be studied, including Word2Vec\cite{mikolov2013efficient}\cite{mikolov2013distributed}\cite{mikolov2013linguistic}.

  In addition to this, other application areas in NLP will be surveyed, including tasks like part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. \cite{socher2011parsing}\cite{luong2013better}\cite{maas2015lexicon}\cite{li2015hierarchical}\cite{collobert2011natural}\cite{pennington2014glove}

% section methodology (end)

\section{Goal} % (fold)
\label{sec:goal}

  The goal of this project is the documentation of the differences, advantages and drawbacks in the domain of feature extraction from text data using neural networks. 

  The project report could serve as a quick cheat-sheet for engineers looking to build a text classification or scoring pipeline, as the comparison would serve to map a use-case to a certain type of feature extraction approach.

% section goal (end)


\section{Document Vectorization} % (fold)
\label{sec:document_vectorization}

  Document vectorization is needed to convert text content into a numeric vector representation that can be utilized as feature vectors, which can then be used to train a machine learning model on. This section talks about a few different statistical methods for computing this feature vector. \cite{SemEvalPaper}

  \subsection{N-gram Model} % (fold)
  \label{sub:n_gram_model}
    N-grams are contiguous sequences of `n' items from a given sequence of text or speech. Given a complete corpus of documents, each tuple of `n' grams, either characters or words are represented by a unique bit in a bit vector, which when aggregated for a body of text, form a sparse vectorized representation of the text in the form of n-gram occurrences.
  
  % subsubsection n_gram_model (end)

  \subsection{TF-IDF Model} % (fold)
  \label{sub:tf_idf_model}
    Term frequency-inverse document frequency (TF-IDF), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus \cite{sparck1972statistical}. The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. It is a bag-of-words model, and doesn't preserve word ordering.
  
  % subsubsection tf_idf_model (end)

  \subsection{Paragraph Vector Model} % (fold)
  \label{sub:paragraph_vectors_doc2vec}

    A Paragraph Vector representation of text is an unsupervised learning algorithm that learns fixed-size vector representations for variable-length pieces of texts such as sentences and documents \cite{le2014distributed}. The vector representations are learned to predict the surrounding words in contexts sampled from the paragraph. In the context of the SemEval headlines, the vector representations were learned for the complete headline.

    Two distinct implementations have gained prominence in the community of late
    \begin{itemize}
      \item 
        Doc2Vec: A Python library implementation in Gensim. \footnote{https://radimrehurek.com/gensim/models/doc2vec.html}.
      \item 
        FastText: A standalone implementation in C++. \cite{bojanowski2016enriching} \cite{joulin2016bag}.
    \end{itemize}
  
  % subsection paragraph_vectors_doc2vec (end)

% section document_vectorization (end)



\section{A Primer of Neural Net Models for NLP} % (fold)
\label{sec:a_primer_of_neural_net_models_for_nlp}

  \begin{itemize}
    \item 
    Fully connected feed-forward neural networks are non-linear learners that can, be used as a drop-in replacement wherever a linear learner is used.
    \item 
    The high accuracy is a result of this non-linearity along with the availability of pre-trained word embeddings.
    \item 
    Multi-layer feed-forward networks can provide competitive results on sentiment classification and factoid question answering
    \item 
    Convolutional and pooling architecture show promising results on many tasks, including document classification, short-text categorization, sentiment classification, relation type classification between entities, event detection, paraphrase identification, semantic role labeling, question answering, predicting box-office revenues of movies based on critic reviews, modeling text interestingness, and modeling the relation between character-sequences and part-of-speech tags.
    \item 
    Convolutional and pooling architectures allow us to encode arbitrary large items as fixed size vectors capturing their most salient features, they do so by sacrificing most of the structural information.
    \item 
    Recurrent and recursive networks allows using sequences and trees and preserve the structural information.
    \item 
    Recurrent models have been shown to produce very strong results for language modeling as well as for sequence tagging, machine translation, dependency parsing, sentiment analysis, noisy text normalization, dialog state tracking, response generation, and modeling the relation between character sequences and part-of-speech tags. 
    \item 
    Recursive models were shown to produce state-of-the-art or near state-of-the-art results for constituency and dependency parse re-ranking, discourse parsing, semantic relation classification, political ideology detection based on parse trees, sentiment classification, target-dependent sentiment classification and question answering.
    \item 
    Convolutional nets seem to work well for summarization related tasks and recurrent/recursive nets seem to work well for language modeling tasks.
  \end{itemize}

% section a_primer_of_neural_net_models_for_nlp (end)


\section{A Neural Probabilistic Language Model} % (fold)
\label{sec:a_neural_probabilistic_language_model}

  \textbf{Background:}
  \begin{itemize}
    \item 
    We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences.
    \item 
    A fundamental problem that makes language modeling and other learning problems difficult is the curse of dimensionality. It is particularly obvious in the case when one wants to model the joint distribution between many discrete random variables (such as words in a sentence, or discrete attributes in a data-mining task)
    \item 
    State-of-the art results are typically obtained using trigrams.
    \item 
    Language generation via subtitution of semantically similar language constructs of existing sentences can be done via shared-parameter multi-layer neural networks.
    \item 
    The objective of this paper is to obtain real-valued vector sequences of words and learn a joint probability function for those sequences of words alongside the feature vector, and hence, jointly learn both the real-value vector representation and the parameters of the probability distribution.
    \item 
    This probability function can be tuned in order to maximize log-likelihood of the training data, while penalizing with a cost function similar to the one used in Ridge regression.
    \item 
    This will ensure that semantically similar words end up with an almost equivalent feature vectors, called learned distributed feature vectors.
    \item 
    A challenge with modeling discrete variables like a sentence structure as opposed to a continuous value is that the continuous valued function can be assumed to have some form of locality, but the same assumption cannot be made in case of discrete functions.
    \item 
    N-gram models try to acheive a statistical modeling of languages by calculating the conditional probabilities of each possible word that can follow a set of $n$ preceding words.
    \item 
    New sequences of words can be generated by effectively gluing together the popular combinations i.e. n-grams with very high frequency counts.
  \end{itemize}

  \textbf{Goal of the paper:}
  Knowing the basic structure of a sentence, we should be able to create a new sentence by replacing parts of the old sentence with interchangeable elements.

  \textbf{Challenges:}
  \begin{itemize}
    \item 
    The main bottleneck for the neural computation is while computing the activations of the output layer
  \end{itemize}

  \textbf{Optimizations:}
  \begin{itemize}
    \item 
    Data parallel processing (different processor working on a different subsets of data) and asynchronous processor usage of shared memory.
  \end{itemize}

% section a_neural_probabilistic_language_model (end)


\section{A Hierarchical Neural Autoencoder for Paragraphs and Documents} % (fold)
\label{sec:a_hierarchical_neural_autoencoder_for_paragraphs_and_documents}

  \begin{itemize}
    \item 
    Attempts to build a paragraph embedding from the underlying word and sentence embeddings, and then proceeds to encode the paragraph embedding in an attempt to reconstruct the original paragraph.
    \item 
    For this to happen, we need to preserve, syntactic, semantic and discourse related properties while creating the embedded representation.
    \item 
    Hierarchical LSTM utilized to preserve sentence structure.
  \end{itemize}

  \textbf{Implementation:}
  \begin{itemize}
    \item 
    An LSTM layer to convert words into a vector representation of a sentence. Another LSTM layer after that to convert multiple sentences into a paragraph.
    \item 
    Parameters are estimated by maximizing likelihood of outputs given inputs, similar to standard sequence-to-sequence models.
    \item 
    Estimations are calculated using softmax functions to maximize the likelihood of the consituent words.
    \item 
    Attention models using the hierarchical autoencoder could be utilized for dialogue systems, since it explicitly models for discourse.
  \end{itemize}

% section a_hierarchical_neural_autoencoder_for_paragraphs_and_documents (end)


\section{Hierarchical Probabilistic Neural Network Language Model} % (fold)
\label{sec:hierarchical_probabilistic_neural_network_language_model}

  \textbf{Goal}
  Implementing a  hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy

  \textbf{Summary:}
  \begin{itemize}
    \item 
    Similar to the previous paper, attempts to tackle the `curse of dimensionality' as in \ref{sec:a_neural_probabilistic_language_model}. 
    \item 
    It attempts to produce a much faster variant.
    \item 
    Back-off n-grams are used for this approach too, and we attempt to learn a real-valued vector representation of each word.
    \item 
    The word embeddings learnt are shared across all the participating nodes in the distributed architecture.
    \item 
    A very important component of the whole model is the choice of the words binary encoding, i.e. of the hierarchical word clustering. In this paper the authors combine empirical statistics with prior knowledge from the WordNet resource.
  \end{itemize}

% section hierarchical_probabilistic_neural_network_language_model (end)


\section{Better Word Representations with Recursive Neural Networks for Morphology} % (fold)
\label{sec:better_word_representations_with_recursive_neural_networks_for_morphology}

  \textbf{Goal:}
  The paper aims to address the inaccuracy in vector representations of complex and rare words, supposedly caused by the lack of relation between morphologically related words. \cite{luong2013better}

  \textbf{Approach:}
  \begin{itemize}
    \item 
    The authors treat each morpheme as a basic unit in the RNNs and construct representations for morpho- logically complex words on the fly from their morphemes. By training a neural language model (NLM) and integrating RNN structures for complex words, they utilize contextual information to learn morphemic semantics and their compositional properties.
    \item 
    Discusses a problem that the Word2Vec syntactic relations like $$x_{apples} - x_{apple} \approx x_{cars} - x_{car}$$ might not hold true if the vector representation of a rare word is inaccurate to begin with.
    \item 
    \texttt{morphoRNN} operates at the morpheme level rather than the word level. An example of the this is illustrated in Figure \ref{fig:rnn-morphology}.
    \begin{figure}[ht]
      \centering
      \includegraphics[width=.4\textwidth]{rnn-morphology}
      \caption{morphoRNN}
      \label{fig:rnn-morphology}
    \end{figure}
    \item 
    Parent words are created by combining a stem vector and an affix vector, as shown in Equation \ref{eqn:parent-vector}.
    \begin{equation} \label{eqn:parent-vector}
      p = f (W_m [x_{stem} ; x_{affix}] + b_m)
    \end{equation}
    \item 
    The cost function is expression in terms of the squared Euclidean loss between the newly constructed representation $p_c(x_i)$ and the reference representation $p_r(x_i)$. The cost function is given in Equation \ref{eqn:cost-function-morphornn}.
    \begin{equation} \label{eqn:cost-function-morphornn}
      J(\theta) = \sum_{i=1}^N (|| p_c(x_i) - p_c(x_i) ||^2_2) + \frac{\lambda}{2} ||\theta||^2_2
    \end{equation}
    \item 
    The paper describes both context sensitive and insensitive versions of the Morphological RNN.
    \item 
    Similar to a typical RNN, the network is trained by computing the activation functions and propagating the errors backward in a forward-backward pass architecture.
  \end{itemize}

  \textbf{Analysis:}
  This RNN model performs better than most of the other neural language model, and could be used to supplement word vectors.
  
% section better_word_representations_with_recursive_neural_networks_for_morphology (end)


\section{Efficient Estimation of Word Representations in Vector Space} % (fold)
\label{sec:efficient_estimation_of_word_representations_in_vector_space}

  \textbf{Goal:}
  The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. This is one of the seminal papers that led to the creation of Word2Vec, which is a state-of-the-art word embeddding tool. \cite{mikolov2013efficient}

  \textbf{Approach:}
  \begin{itemize}
    \item 
    The ideas presented in this paper build on the previous ideas presented by Bengio et.al.\cite{bengio2003neural}
    \item 
    The objective was to obtain high-quality word embeddings that capture the syntactic and semantic characteristics of words in a manner that allows algebraic operations to proxy the distances in vector space.
    $$man - woman = king - queen$$ or $$tell - told = walk - walked$$
    \item 
    The training time here scales with the dimensionality of the learned feature vectors and not on the volume of training data.
    \item 
    The approach attempts to find a distributed vector representation of values as opposed to a continuous representation of values as computed by methods like LSA and LDA.
    \item 
    The models are trained using stochastic gradient descent and backpropagation.
    \item 
    The RNN models are touted to be able to have an inherently better representation of sentence structure for complex patterns, without the need to specify context length.
    \item 
    To allow for the distributed training of the data, the framework DistBelief was used with mutiple replicas of the model. Adagrad was utilized for asynchronous gradient descent.
    \item 
    Two distint models were conceptualized for the training of the word vectors based on context, both of which are continous and distributed representations of words.
    \begin{itemize}
      \item 
      Continuous Bag-of-Words model: This model uses the context of a word i.e. the words that precede and follow it, to be able to predict the current word.
      \item 
      Skip-gram model: This model uses the current word to be able to predict the context it appeared in.
    \end{itemize}
  \end{itemize}

  \textbf{Challenges:}
  \begin{itemize}
    \item 
    The computational complexity that arises at the fully-connected output layer of the neural network is the dominant part of the computation. A couple of methods suggested to mitigate this is to use hierarchical versions of the softmax output activation units, or to refrain from performing normalization at the final layer altogether.
  \end{itemize}

  The experimental results show that the CBOW and Skipgram models consistently out-performed the then, state-of-the-art models. It was also observed that after a point, increasing the dimensions and the size of the data began providing diminishing returns.

% section efficient_estimation_of_word_representations_in_vector_space (end)


\section{Distributed Representations of Words and Phrases and their Compositionality} % (fold)
\label{sec:distributed_representations_of_words_and_phrases_and_their_compositionality}

  \textbf{Goal:}
  This paper builds upon the idea of the Word2Vec skip-gram model, and presents optimizations in terms of quality of the word embeddings as well as speed-ups while training. It also proposes an alternative to the hierarchical softmax final layer, called negative sampling.

  \textbf{Approach:}
  \begin{itemize}
    \item 
    
  \end{itemize}


% section distributed_representations_of_words_and_phrases_and_their_compositionality (end)


\newpage

\bibliographystyle{unsrt}
\bibliography{cs698_project_report}

\end{document}
