\documentclass[a4paper]{article}

%% Language and font encodings
% \usepackage[english]{babel}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{indentfirst}
\graphicspath {{images/}}

\title{CS698 - Winter 2017 - Assignment 2}
\author{Vineet John (v2john@uwaterloo.ca)}
\date{}
	
\begin{document}
\maketitle

\renewcommand\thesubsection{\alph{subsection}}

\section{Classification Algorithms}

\subsection{Mixture of Gaussians}
The cross-validation accuracy for each run is as reporting in below table. The average accuracy is \textbf{0.8677}

\begin{center} 
	\begin{tabular}{ |c|c| } 
		\hline
		\textbf{Cross-validation Run} & \textbf{Accuracy} \\
		\hline
		\hline
		Set 1  & 0.883 \\
		\hline
		Set 2  & 0.847 \\
		\hline
		Set 3  & 0.856 \\
		\hline
		Set 4  & 0.91 \\
		\hline
		Set 5  & 0.865 \\
		\hline
		Set 6  & 0.892 \\
		\hline
		Set 7  & 0.838 \\
		\hline
		Set 8  & 0.892 \\
		\hline
		Set 9  & 0.82 \\
		\hline
		Set 10 & 0.874 \\
		\hline
	\end{tabular}
\end{center}

The accuracy while training and evaluating on the complete dataset is \textbf{0.889}

\vspace{3mm}

\textbf{Parameters}: (after training on the full dataset)

\vspace{3mm}

Variance for Label 5: 
$\begin{tabular}{  c c c c c  }
    [ 19.2166544  & 19.5197397  & 22.3296676  & 21.01331568 & 18.00399274 \\
      22.0238518  & 22.35962787 & 17.44205621 & 19.18318209 & 21.1330423 \\
      18.96626608 & 20.90767319 & 22.2647079  & 21.40778601 & 19.72100522 \\
      18.71266657 & 17.41137326 & 21.72900705 & 20.09414169 & 23.6765684 \\
      20.54920619 & 18.85736335 & 18.15233891 & 17.36921895 & 18.76942496 \\
      20.48911561 & 20.32647275 & 22.31978221 & 23.72603783 & 22.93736204 \\
      17.83163781 & 18.66148363 & 18.86834748 & 20.82754043 & 22.61927699 \\
      23.28439365 & 23.40934255 & 23.05357009 & 20.53472311 & 16.19298246 \\
      18.6255555  & 18.35211981 & 19.02730498 & 20.88145387 & 22.96913393 \\
      22.07457367 & 20.56364677 & 17.42282173 & 17.01701411 & 20.14538677 \\
      22.14428476 & 22.64114714 & 22.89979889 & 22.91875542 & 21.39598437 \\
      18.35453966 & 18.40332892 & 17.97829009 & 23.35579863 & 20.62316509 \\
      22.78647831 & 22.69232027 & 19.61217115 & 20.13740129] & \\
\end{tabular}$

\vspace{3mm}

Variance for Label 6: 
$\begin{tabular}{  c c c c c  }
    [ 20.64206998 & 20.05145611 & 20.9265935  & 21.39779529 & 22.38896034 \\
      19.94142694 & 21.29769698 & 18.33850989 & 18.45884704 & 18.50073974 \\
      22.69260423 & 20.52305114 & 21.57993521 & 18.44925462 & 19.53336879 \\
      18.50814356 & 18.55568341 & 19.22506406 & 22.45103708 & 23.08554852 \\
      19.18702638 & 19.39292495 & 17.93102226 & 19.37047817 & 17.5607536 \\
      19.90213219 & 19.71137005 & 23.68418186 & 21.80484456 & 20.4728102 \\
      19.79193541 & 18.46132895 & 18.95928702 & 21.84798466 & 19.3203855 \\
      20.9425164  & 21.61415655 & 22.20206933 & 20.91113153 & 16.83199407 \\
      18.94408613 & 21.43140421 & 20.84368805 & 23.54909507 & 23.48812713 \\
      21.29490242 & 22.94041483 & 18.18635917 & 18.22072556 & 20.05706458 \\
      21.80482522 & 21.61760544 & 22.97601251 & 21.82179568 & 22.54277748 \\
      20.54191687 & 19.63753807 & 19.4810553  & 21.52908508 & 21.37512933 \\
      19.53557028 & 21.32065948 & 23.18661058 & 20.33026801]& \\
\end{tabular}$

\vspace{3mm}

$\bar{w} =
	\begin{tabular}{  c c c c c c  } 
		[-0.00039919 &  0.01608376 &  0.03035347 &  0.01290756 &  0.05312515 &  0.15887134 \\
		  0.02267963 &  0.03617127 &  0.08186617 & -0.02446763 &  0.11650799 &  0.0121413 \\
		  0.02111923 &  0.15654718 &  0.0109459  & -0.0506001  &  0.07229666 &  0.06398694 \\
		  0.00981228 & -0.0201207  & -0.01176678 & -0.05647826 & -0.03382154 &  0.02534816 \\
		  0.05242411 &  0.01178864 & -0.02082784 &  0.01802898 &  0.08832164 & -0.00688474 \\
		 -0.06683389 &  0.00220141 &  0.07158928 & -0.00998629 & -0.06456113 & -0.04018647 \\
		 -0.03329252 & -0.08199461 & -0.03852807 & -0.04455228 & -0.0420453  & -0.00382048 \\
		 -0.21711455 & -0.08211564 & -0.00247919 &  0.02724546 &  0.02489109 & -0.04608706 \\
		  0.03292018 & -0.09936085 & -0.07256317 & -0.05318881 &  0.06202186 & -0.03173325 \\
		 -0.07233379 &  0.00984539 & -0.02589944 & -0.07307696 &  0.09056223 &  0.05856089 \\
		 -0.01847382 & -0.04885932 &  0.00456147 &  0.01576207] & & \\
	\end{tabular}$

\vspace{3mm}
$w_0 = 0.105720724448$

\vspace{3mm}

The code is provided in the folder `mixture-of-gaussians'. Run commands are present in the README.md file.

\newpage

\subsection{Logistic Regression} \label{logistic_regression_section}
The cross-validation accuracy for each run is as reporting in below table. The average accuracy is \textbf{0.8632}

\begin{center} 
	\begin{tabular}{ |c|c| } 
		\hline
		\textbf{Cross-validation Run} & \textbf{Accuracy} \\
		\hline
		\hline
		Set 1 & 0.901 \\
		\hline
		Set 2 & 0.829 \\
		\hline
		Set 3 & 0.847 \\
		\hline
		Set 4 & 0.928 \\
		\hline
		Set 5 & 0.865 \\
		\hline
		Set 6 & 0.91 \\
		\hline
		Set 7 & 0.802 \\
		\hline
		Set 8 & 0.865 \\
		\hline
		Set 9 & 0.82 \\
		\hline
		Set 10 & 0.865 \\
		\hline
	\end{tabular}
\end{center}

The accuracy while training and evaluating on the complete dataset is \textbf{0.887}

\vspace{3mm}

\textbf{Parameters}: (after training on the full dataset)

\vspace{3mm}

$\bar{w} =
	\begin{tabular}{  c c c c c c  } 
		[ 1.34693295e-01 & -2.95016036e-02 &  1.42379140e-02 &  3.48930440e-02 \\
		  1.52217181e-02 &  4.42212544e-02 &  1.82674123e-01 &  3.78298196e-02 \\
		  6.30007348e-02 &  9.39523892e-02 & -4.72307408e-03 &  1.04525881e-01 \\
		  1.67596878e-02 &  2.59975589e-02 &  1.92852432e-01 &  3.45685863e-03 \\
		 -5.14217586e-02 &  5.28535920e-02 &  7.52766694e-02 & -7.37422660e-03 \\
		 -1.98994419e-02 &  6.89265132e-03 & -9.68883208e-02 & -2.20082922e-02 \\
		  5.21423842e-02 &  6.08412098e-02 &  1.92349015e-02 & -1.30316116e-02 \\
		  3.17330368e-02 &  8.94940252e-02 & -3.81283062e-03 & -1.01410064e-01 \\
		  2.70688864e-02 &  1.34839872e-01 & -2.18704999e-02 & -6.84437721e-02 \\
		 -3.24198044e-02 & -1.67515348e-02 & -7.02219612e-02 & -4.06115529e-02 \\
		 -5.79807551e-02 & -2.24190448e-02 & -1.40471238e-02 & -2.30941396e-01 \\
		 -9.26106434e-02 &  6.19529070e-03 & -9.13217080e-05 &  1.40258620e-02 \\
		 -9.32931714e-02 &  1.19996489e-02 & -1.50596723e-01 & -8.91628888e-02 \\
		 -7.42472890e-02 &  6.32482144e-02 & -3.47387718e-02 & -8.47732301e-02 \\
		 -3.01988102e-04 & -3.36763096e-02 & -7.03418737e-02 &  1.12675658e-01 \\
		  1.00865177e-01 & -4.32176059e-02 & -4.85711240e-02 & -5.77789904e-03 \\
		  3.50403361e-02] & & & \\
	\end{tabular}$

\vspace{3mm}

$w_0$ is the first term in the above vector. The code is provided in the folder `logistic-regression'. Run commands are present in the README.md file.

\newpage

\subsection*{Discussion of results}
\subsubsection*{Parameterizations and Objectives - Mixture of Gaussians vs. Logistic Regression}
The main difference between Mixture of Gaussians and Logistic Regression as classification approaches, is the way in which the probabilities of a hypothesis being true are modelled. 

Mixture of Gaussians attempts to model this as a joint probability distribution, and hence, is a generative model. It can predict the posterior probability of each class, given a new data point by relying on Bayesian inference. 

In contrast, Logistic regression uses conditional probabilities, and is hence, a discriminative model. It is used to directly predict the posterior probability through an iterative method of gradient descent to coverge at the minima of the convex loss function.

\subsubsection*{Separators - Mixture of Gaussians \& Logistic Regression vs. K Nearest Neighbours}
The choice of classifier depends on the how the data that is intended to be classified is separated.
A dataset is said to be linear separable if there exists a class-separating hyperplane. If there exists such a hyperplane, then having a classification method with a a linear decision boundary will provide better accuracy results.
On the other hand, if the convex hulls of the 2 classes in n-dimensional space overlap, and the dataset isn't linearly separable, then better classification results will be obtained by using a non-linear classifier like KNN.

\newpage

\section{Linear Separability}

\subsection{Threshold perceptron - Activation function encoding}
Assuming that the input variables are $x_1$ and $x_2$, a direct encoding of the functions 'AND' and 'OR' is possible, with a compounding of these encodings being used to produce, 'XOR' and 'IFF'
\begin{itemize}
	\item AND: The activation weights are $w_0 = -1.5$; $w_1 = 1;$ $w_2 = 1$
	\item OR: The activation weights are $w_0 = -0.5$; $w_1 = 1;$ $w_2 = 1$
	\item XOR: 
		\begin{itemize}
			\item The mapping functions are $\phi_1(x) = x_1 \land x_2$ and $\phi_2(x) = x_1 \lor x_2$. The input variables are $\phi_1(x)$ and $\phi_2(x)$.
			\item The activation weights are $w_0 = -0.5$; $w_1 = -1$; $w_2 = 1$
		\end{itemize}
	\item IFF: 
		\begin{itemize}
			\item The mapping function is $\phi_3(x) = x_1 \oplus x_2$. The input variable is $\phi_3(x)$.
			\item The activation weights are $w_0 = 0.5$; $w_1 = -1$
		\end{itemize}
\end{itemize}
All of the weights are illustrated in Figure \ref{fig_activation_function_weights}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/activation_functions.png}
    \caption{Activation Function Weights}
    \label{fig_activation_function_weights}
\end{figure*}

\newpage

\subsection{Logistic Regression Experiment - Linear Separability}
When a given dataset is linearly separable, then the step-size becomes zero and the learning function converges. For the experimental setup, the choice can be made to simply not regularize the loss function that is being minimized. The learning algorithm is now no longer constrained by excessively large values for weights. 

The experiment run involves running logistic regression on the dataset and observing if the step begins to fluctuate back and forth after a few iterations of Newton's Algorithm. The same logistic regression experiment, as described in \ref{logistic_regression_section} has been used here, without any weight penalization parameters($\lambda$). 

\subsubsection*{Experimental Setup}
\begin{itemize}
    \item Run Newton's algorithm iterations over the given dataset an arbitrarily high number of times.
	\item Measure the norm of the step vector ($H^{-1} \Delta L(w)$) for every iteration. (The step vector is rounded off to 4 decimals)
    \item If the norm doesn't progressively reduce and become zero, this implies that the step vector is not converging to a minima.
\end{itemize}

\subsubsection*{Observation}
After 6 iterations of Newton's algorithm, the iterative step norm converges to 0.

\subsubsection*{Conclusion}
This leads to the conclusion that the data points in the given set are \textbf{linearly separable}.

\end{document}
