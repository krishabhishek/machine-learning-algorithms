\documentclass[a4paper]{article}

%% Language and font encodings
% \usepackage[english]{babel}
% \usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{indentfirst}
\graphicspath {{images/}}

\title{CS698 - Winter 2017 - Assignment 2}
\author{Vineet John (v2john@uwaterloo.ca)}
\date{}
	
\begin{document}
\maketitle

\renewcommand\thesubsection{\alph{subsection}}

\section{Classification Algorithms}

\subsection{Mixture of Gaussians}
The cross-validation accuracy for each run is as reporting in below table. The average accuracy is \textbf{0.8677}

\begin{center} 
	\begin{tabular}{ |c|c| } 
		\hline
		\textbf{Cross-validation Run} & \textbf{Accuracy} \\
		\hline
		\hline
		Set 1  & 0.883 \\
		\hline
		Set 2  & 0.847 \\
		\hline
		Set 3  & 0.856 \\
		\hline
		Set 4  & 0.91 \\
		\hline
		Set 5  & 0.865 \\
		\hline
		Set 6  & 0.892 \\
		\hline
		Set 7  & 0.838 \\
		\hline
		Set 8  & 0.892 \\
		\hline
		Set 9  & 0.82 \\
		\hline
		Set 10 & 0.874 \\
		\hline
	\end{tabular}
\end{center}

The accuracy while training and evaluating on the complete dataset is \textbf{0.889}

\vspace{5mm}

\textbf{Parameters}: (after training on the full dataset)

\vspace{5mm}

$\bar{w} =
	\begin{tabular}{  c c c c c c  } 
		[-0.00039919 &  0.01608376 &  0.03035347 &  0.01290756 &  0.05312515 &  0.15887134 \\
		  0.02267963 &  0.03617127 &  0.08186617 & -0.02446763 &  0.11650799 &  0.0121413 \\
		  0.02111923 &  0.15654718 &  0.0109459  & -0.0506001  &  0.07229666 &  0.06398694 \\
		  0.00981228 & -0.0201207  & -0.01176678 & -0.05647826 & -0.03382154 &  0.02534816 \\
		  0.05242411 &  0.01178864 & -0.02082784 &  0.01802898 &  0.08832164 & -0.00688474 \\
		 -0.06683389 &  0.00220141 &  0.07158928 & -0.00998629 & -0.06456113 & -0.04018647 \\
		 -0.03329252 & -0.08199461 & -0.03852807 & -0.04455228 & -0.0420453  & -0.00382048 \\
		 -0.21711455 & -0.08211564 & -0.00247919 &  0.02724546 &  0.02489109 & -0.04608706 \\
		  0.03292018 & -0.09936085 & -0.07256317 & -0.05318881 &  0.06202186 & -0.03173325 \\
		 -0.07233379 &  0.00984539 & -0.02589944 & -0.07307696 &  0.09056223 &  0.05856089 \\
		 -0.01847382 & -0.04885932 &  0.00456147 &  0.01576207] & & \\
	\end{tabular}$

\vspace{5mm}
$w_0 = 0.105720724448$

\begin{center}
	\noindent\rule{4cm}{0.4pt}
\end{center}

\subsection{Logistic Regression}
The cross-validation accuracy for each run is as reporting in below table. The average accuracy is \textbf{0.8632}

\begin{center} 
	\begin{tabular}{ |c|c| } 
		\hline
		\textbf{Cross-validation Run} & \textbf{Accuracy} \\
		\hline
		\hline
		Set 1 & 0.901 \\
		\hline
		Set 2 & 0.829 \\
		\hline
		Set 3 & 0.847 \\
		\hline
		Set 4 & 0.928 \\
		\hline
		Set 5 & 0.865 \\
		\hline
		Set 6 & 0.91 \\
		\hline
		Set 7 & 0.802 \\
		\hline
		Set 8 & 0.865 \\
		\hline
		Set 9 & 0.82 \\
		\hline
		Set 10 & 0.865 \\
		\hline
	\end{tabular}
\end{center}

The accuracy while training and evaluating on the complete dataset is \textbf{0.889}

\vspace{5mm}

\textbf{Parameters}: (after training on the full dataset)

\vspace{5mm}

$\bar{w} =
	\begin{tabular}{  c c c c c c  } 
		[ 1.34693295e-01 & -2.95016036e-02 &  1.42379140e-02 &  3.48930440e-02 \\
		  1.52217181e-02 &  4.42212544e-02 &  1.82674123e-01 &  3.78298196e-02 \\
		  6.30007348e-02 &  9.39523892e-02 & -4.72307408e-03 &  1.04525881e-01 \\
		  1.67596878e-02 &  2.59975589e-02 &  1.92852432e-01 &  3.45685863e-03 \\
		 -5.14217586e-02 &  5.28535920e-02 &  7.52766694e-02 & -7.37422660e-03 \\
		 -1.98994419e-02 &  6.89265132e-03 & -9.68883208e-02 & -2.20082922e-02 \\
		  5.21423842e-02 &  6.08412098e-02 &  1.92349015e-02 & -1.30316116e-02 \\
		  3.17330368e-02 &  8.94940252e-02 & -3.81283062e-03 & -1.01410064e-01 \\
		  2.70688864e-02 &  1.34839872e-01 & -2.18704999e-02 & -6.84437721e-02 \\
		 -3.24198044e-02 & -1.67515348e-02 & -7.02219612e-02 & -4.06115529e-02 \\
		 -5.79807551e-02 & -2.24190448e-02 & -1.40471238e-02 & -2.30941396e-01 \\
		 -9.26106434e-02 &  6.19529070e-03 & -9.13217080e-05 &  1.40258620e-02 \\
		 -9.32931714e-02 &  1.19996489e-02 & -1.50596723e-01 & -8.91628888e-02 \\
		 -7.42472890e-02 &  6.32482144e-02 & -3.47387718e-02 & -8.47732301e-02 \\
		 -3.01988102e-04 & -3.36763096e-02 & -7.03418737e-02 &  1.12675658e-01 \\
		  1.00865177e-01 & -4.32176059e-02 & -4.85711240e-02 & -5.77789904e-03 \\
		  3.50403361e-02] & & & \\
	\end{tabular}$

\begin{center}
	\noindent\rule{4cm}{0.4pt}
\end{center}

\subsection{Discussion of results}
\subsubsection*{Parameterizations and Objectives - Mixture of Gaussians vs. Logistic Regression}
The main difference between Mixture of Gaussians and Logistic Regression as classification approaches, is the way in which the probabilities of a hypothesis being true are modelled. 

Mixture of Gaussians attempts to model this as a joint probability distribution, and hence, is a generative model. It can predict the posterior probability of each class, given a new data point by relying on Bayesian inference. 

In contrast, Logistic regression uses conditional probabilities, and is hence, a discriminative model. It is used to directly predict the posterior probability through an iterative method of gradient descent to coverge at the minima of the convex loss function.

\subsubsection*{Separators - Mixture of Gaussians \& Logistic Regression vs. K Nearest Neighbours}
The choice of classifier depends on the how the data that is intended to be classified is separated.
A dataset is said to be linear separable if there exists a class-separating hyperplane. If there exists such a hyperplane, then having a classification method with a a linear decision boundary will provide better accuracy results.
On the other hand, if the convex hulls of the 2 classes in n-dimensional space overlap, and the dataset isn't linearly separable, then better classification results will be obtained by using a non-linear classifier like KNN.

\newpage

\section{Linear Separability}

\subsection{Threshold perceptron - Activation function encoding}
Assuming that the input variables are $x_1$ and $x_2$, a direct encoding of the functions 'AND' and 'OR' is possible, with a compounding of these encodings being used to produce, 'XOR' and 'IFF'
\begin{itemize}
	\item AND: The activation weights are $w_0 = -1.5$; $w_1 = 1;$ $w_2 = 1$
	\item OR: The activation weights are $w_0 = -0.5$; $w_1 = 1;$ $w_2 = 1$
	\item XOR: 
		\begin{itemize}
			\item The mapping functions are $\phi_1(x) = x_1 \land x_2$ and $\phi_2(x) = x_1 \lor x_2$. The input variables are $\phi_1(x)$ and $\phi_2(x)$.
			\item The activation weights are $w_0 = -0.5$; $w_1 = -1$; $w_2 = 1$
		\end{itemize}
	\item IFF: 
		\begin{itemize}
			\item The mapping function is $\phi_3(x) = x_1 \oplus x_2$. The input variable is $\phi_3(x)$.
			\item The activation weights are $w_0 = 0.5$; $w_1 = -1$
		\end{itemize}
\end{itemize}
All of the weights are illustrated in Figure \ref{fig_activation_function_weights}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/activation_functions.png}
    \caption{Activation Function Weights}
    \label{fig_activation_function_weights}
\end{figure*}

\newpage

\subsection{Logistic Regression Experiment - Linear Separability}

\end{document}
