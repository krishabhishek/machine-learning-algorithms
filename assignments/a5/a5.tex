\documentclass[parskip=half]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath {{images/}}
\hypersetup{
    colorlinks=true,
    citecolor=blue
}

\renewcommand\thesubsection{\thesection.\alph{subsection}}
\DeclareMathOperator*{\argmax}{argmax}

\begin{document}


\title{CS698 - Assignment 5}
\subtitle{Winter 2017}
\author{
    Vineet John\\
    \texttt{v2john@uwaterloo.ca}
}
\date{\today}
\maketitle


\section{Tensorflow - MNIST} % (fold)
\label{sec:tensorflow_mnist}

    \subsection{Softmax vs. CNN vs. Fully Connected} % (fold)
    \label{sub:softmax_vs_cnn_vs_fully_connected}

        The result for the vanilla experiments are given in Table \ref{tab:accuracy_comparisons_network_types}.
        \begin{table}[th]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Neural Net Type} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                Softmax Regression Network & 0.92 & 0.9205 \\
                \hline
                Convolutional Neural Network & 0.98 & 0.9664 \\
                \hline
                Fully-connected Feedforward Neural Network & 0.8727 & 0.8838 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Network Types}
            \label{tab:accuracy_comparisons_network_types}
        \end{table}

        \textbf{Discussion:}\\
        The reason than the CNN performs the best is because it considers patches of the image rather than the intensities of sequential pixels. Since the CNN preserves this 2-D information structure, it performs the best of the lot.
    
    % subsection softmax_vs_cnn_vs_fully_connected (end)

    \subsection{CNN - ReLU vs. Sigmoid Units} % (fold)
    \label{sub:cnn_relu_vs_sigmoid_units}

        The results of using different activation functions in Rectified linear units and Sigmoid units are illustrated in Table \ref{tab:accuracy_comparisons_unit_types}.

        \begin{table}[th]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Neural Net Unit Type} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                Rectified Linear Units & 0.98 & 0.9664 \\
                \hline
                Sigmoid Units & 0.76 & 0.7454 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Unit Types}
            \label{tab:accuracy_comparisons_unit_types}
        \end{table}

        \textbf{Discussion:}\\
        ReLUs were found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid function. It is argued that this is due to its linear, non-saturating form. (Saturation happens for the sigmoid function at the tail of it's outputs 0 and 1, where the gradient is almost 0.)
    
    % subsection cnn_relu_vs_sigmoid_units (end)

    \subsection{Tweaking Dropout Level} % (fold)
    \label{sub:tweaking_dropout_level}

        The results of varying the dropout factor for the CNN training are illustrated in Table \ref{tab:accuracy_comparisons_dropout}.

        \begin{table}[th]
            \centering
            \begin{tabular}{| r | r | r |}
            \hline
            \textbf{Dropout Factor} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                0.25 & 0.92 & 0.9519 \\
                \hline
                0.5 & 0.98 & 0.9664 \\
                \hline
                0.75 & 0.98 & 0.9708 \\
                \hline
                1 & 0.96 & 0.9672 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Dropout}
            \label{tab:accuracy_comparisons_dropout}
        \end{table}

        \textbf{Discussion:}\\
        Unit dropout is used primarily to avoid overfitting and help the neural network become invariant to the training cycles. This is intended to make the network robust to predicting new data-points that are part of the test set. This explains why additional dropbout corresponds to an increasing accuracy score for the testing data. The best \texttt{keep\_prob} rate for this particular dataset and CNN config appears to be 0.75.

    
    % subsection tweaking_dropout_level (end)

    \subsection{Tweaking Network Architecture} % (fold)
    \label{sub:tweaking_network_architecture}

        The results of varying the network architecture in the fully connected feedforward neural netword are illustrated in Table \ref{tab:accuracy_comparisons_network_architecture}.

        \begin{table}[th]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Network architecture} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                (150) & 0.8712 & 0.8814 \\
                \hline
                (128) -> (32) & 0.8727 & 0.8838 \\
                \hline
                (85) -> (40) -> (25) & 0.8566 & 0.8611 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Network Architecture}
            \label{tab:accuracy_comparisons_network_architecture}
        \end{table}

        \textbf{Discussion:}\\
        Both the training and test accuracies have a very low factor of deviation when it comes to varying the depth of the network. However, the number of neurons is the same for each experiment and each layer is fully connected to the next layer in the network. It seems to be that the 2 layer network performs marginally better than the other. 
    
    % subsection tweaking_network_architecture (end)

% section tensorflow_mnist (end)


\section{Object Recognition} % (fold)
\label{sec:object_recognition}

    \textbf{Assumptions}
    \begin{itemize}
        \item 
        The original image dimensions are $m*m$.
        \item 
        As stated in the question is that the foreground objects is away from the image borders by at least 10px always.
        \item 
        The weights of the model are fixed, and both the original and translated images are evaluated purely on the basis of their neuron activates at the penultimate layer.
    \end{itemize}

    \subsection{Single Convolutions} % (fold)
    \label{sub:single_convolutions}

        No, the feature representation \textbf{will not be translation invariant}.

        \textbf{Justification:}\\
        The original and translation images are convolved over a $5*5$ patch in the image.
        The translation of the image for through a given number of pixels `n', results in a representation of dimensions $(m-4)*(m-4)$.

        However, the translation results in a different set of activated units for the original vs. the translated image, thus, leading to the conclusion that the neural networks' representation will not be translation invariant.

        \textbf{Counter-example:}\\
        Consider the original image having a final-layer representation as below:
        \begin{equation*}
            \begin{bmatrix}
                0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0
            \end{bmatrix}
        \end{equation*}
        As the translated image has not aggregated in a disjoint manner such as to ensure that the final activation units do not change, an example of the final-layer can be given as follows:
        \begin{equation*}
            \begin{bmatrix}
                0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0
            \end{bmatrix}
        \end{equation*}
        Since the output being fed to the softmax layer is different, this CNN setup is \textbf{not translational invariant} to any pixel translations.


    % subsection single_convolutions (end)


    \subsection{Convolution and Max Pooling} % (fold)
    \label{sub:convolution_and_max_pooling}

        No, the feature representation \textbf{will not be translation invariant}.

        \textbf{Justification:}\\
        Although there are scenarios where the network will remain translation invariant, this justification focuses on a failure scenario, where the representation varies due to translation.

        \textbf{Counter-example:}\\
        Consider the original image having a post convnet layer representation as below. The vertical separator delineates  
        \[
            \left[
                \begin{array}{cccc|cccc}
                    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                    0 & 1 & 6 & 2 & 0 & 0 & 0 & 0 \\
                    0 & 3 & 1 & 3 & 1 & 0 & 0 & 0 \\
                    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                \end{array}
            \right]
        \]
        This will be max-pooled to create the activation units
        \[
            \left[
                \begin{array}{cc}
                    6 & 1
                \end{array}
            \right]
        \]

        Now consider that this representation has been translated 2px to the right such that it's representation is as below.
        \[
            \left[
                \begin{array}{cccc|cccc}
                    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                    0 & 0 & 0 & 1 & 6 & 2 & 0 & 0 \\
                    0 & 0 & 0 & 3 & 1 & 3 & 1 & 0 \\
                    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                \end{array}
            \right]
        \]
        This will be max-pooled to create the activation units, which are different from those created by the original image.
        \[
            \left[
                \begin{array}{cc}
                    3 & 6
                \end{array}
            \right]
        \]

        Since the output being fed to the softmax layer is different, this CNN setup is not translational invariant to this pixel translation. However, there are certain translations that the network can still be invariant too. The answer is based on it's response to any translation that is within 10px, and the invariance property is not satisfied for the above illustrated translation.

    % subsection convolution_and_max_pooling (end)


% section object_recognition (end)

\end{document}
