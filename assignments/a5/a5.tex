\documentclass[parskip=half]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath {{images/}}
\hypersetup{
    colorlinks=true,
    citecolor=blue
}

\renewcommand\thesubsection{\thesection.\alph{subsection}}
\DeclareMathOperator*{\argmax}{argmax}

\begin{document}


\title{CS698 - Assignment 5}
\subtitle{Winter 2017}
\author{
    Vineet John\\
    \texttt{v2john@uwaterloo.ca}
}
\date{\today}
\maketitle


\section{Tensorflow - MNIST} % (fold)
\label{sec:tensorflow_mnist}

    \subsection{Softmax vs. CNN vs. Fully Connected} % (fold)
    \label{sub:softmax_vs_cnn_vs_fully_connected}

        The result for the vanilla experiments are given in Table \ref{tab:accuracy_comparisons_network_types}.
        \begin{table}[th]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Neural Net Type} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                Softmax Regression Network & 0.92 & 0.9205 \\
                \hline
                Convolutional Neural Network & 0.98 & 0.9664 \\
                \hline
                Fully-connected Feedforward Neural Network & 0.8727 & 0.8838 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Network Types}
            \label{tab:accuracy_comparisons_network_types}
        \end{table}

        \textbf{Discussion:}\\
        The reason than the CNN performs the best is because it considers patches of the image rather than the intensities of sequential pixels. Since the CNN preserves this 2-D information structure, it performs the best of the lot.
    
    % subsection softmax_vs_cnn_vs_fully_connected (end)

    \subsection{CNN - ReLU vs. Sigmoid Units} % (fold)
    \label{sub:cnn_relu_vs_sigmoid_units}

        The results of using different activation functions in Rectified linear units and Sigmoid units are illustrated in Table \ref{tab:accuracy_comparisons_unit_types}.

        \begin{table}[th]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Neural Net Unit Type} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                Rectified Linear Units & 0.98 & 0.9664 \\
                \hline
                Sigmoid Units & 0.76 & 0.7454 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Unit Types}
            \label{tab:accuracy_comparisons_unit_types}
        \end{table}

        \textbf{Discussion:}\\
        ReLUs were found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid function. It is argued that this is due to its linear, non-saturating form. (Saturation happens for the sigmoid function at the tail of it's outputs 0 and 1, where the gradient is almost 0.)
    
    % subsection cnn_relu_vs_sigmoid_units (end)

    \subsection{Tweaking Dropout Level} % (fold)
    \label{sub:tweaking_dropout_level}

        The results of varying the dropout factor for the CNN training are illustrated in Table \ref{tab:accuracy_comparisons_dropout}.

        \begin{table}[th]
            \centering
            \begin{tabular}{| r | r | r |}
            \hline
            \textbf{Dropout Factor} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                0.25 & 0.92 & 0.9519 \\
                \hline
                0.5 & 0.98 & 0.9664 \\
                \hline
                0.75 & 0.98 & 0.9708 \\
                \hline
                1 & 0.96 & 0.9672 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Dropout}
            \label{tab:accuracy_comparisons_dropout}
        \end{table}

        \textbf{Discussion:}\\
        Unit dropout is used primarily to avoid overfitting and help the neural network become invariant to the training cycles. This is intended to make the network robust to predicting new data-points that are part of the test set. This explains why additional dropbout corresponds to an increasing accuracy score for the testing data. The best \texttt{keep\_prob} rate for this particular dataset and CNN config appears to be 0.75.

    
    % subsection tweaking_dropout_level (end)

    \subsection{Tweaking Network Architecture} % (fold)
    \label{sub:tweaking_network_architecture}

        The results of varying the network architecture in the fully connected feedforward neural netword are illustrated in Table \ref{tab:accuracy_comparisons_network_architecture}.

        \begin{table}[th]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Network architecture} & \textbf{Training Accuracy} & \textbf{Testing Accuracy} \\
            \hline
                \hline
                (150) & 0.8712 & 0.8814 \\
                \hline
                (128) -> (32) & 0.8727 & 0.8838 \\
                \hline
                (85) -> (40) -> (25) & 0.8566 & 0.8611 \\
            \hline
            \end{tabular}
            \caption{Accuracy Comparisons - Network Architecture}
            \label{tab:accuracy_comparisons_network_architecture}
        \end{table}

        \textbf{Discussion:}\\
        Both the training and test accuracies have a very low factor of deviation when it comes to varying the depth of the network. However, the number of neurons is the same and each layer is fully connected to the next layer in the network.
    
    
    % subsection tweaking_network_architecture (end)

% section tensorflow_mnist (end)


\section{Object Recognition} % (fold)
\label{sec:object_recognition}

    \subsection{Single Convolutions} % (fold)
    \label{sub:single_convolutions}

        Yes, the feature representation will be translation invariant.

        \textbf{Justification:}\\
        The image is always 10 pixels away from the nearest border. This fact itself ensures that the object in the foreground, can remain translation invariant for upto a single convolution of upto $10*10$ pixels.

        To explain this more intuitively, the foreground image has a $(n-10)*(n-10)$ pixel area of possible translation.

        The resulting feature set of the single convolution layer would be $(n-4)*(n-4)$ pixels in dimension. This ensures that there is no loss of data, since the image itself exists on a smaller subset of that dimension ($(n-10)*(n-10)$).
    
    % subsection single_convolutions (end)


    \subsection{Double Convolution and Max Pooling} % (fold)
    \label{sub:double_convolution_and_max_pooling}

        No, the feature representation will not be translation invariant.

        This is because, each feature of the final representation will only be invariant if it abstracts the information of pixels at most 10px away from itself. Let us consider the steps involved with the Double convolution and max pooling layers.

        \textbf{Justification:}\\
        \begin{itemize}
            \item 
            Conv. Layer 1 (Convolution:5x5, Stride:1): Each resulting feature after this step encapsulates information of pixels that are at most 5 pixels away, inclusive of itself. 
            \item 
            Conv. Layer 2 (Convolution:5x5, Stride:1): Each resulting feature after this step encapsulates information of pixels that are at most $5 + 4 = 9$ pixels away, inclusive of itself.
            \item 
            Pooling layer (Convolution:4x4, Stride:4): Each resulting feature after this step encapsulates information of pixels that are at most $9 + 3 = 12$ pixels away, inclusive of itself.
        \end{itemize}

        This shows that the final representation of the image has every pixel that is representative of a 12x12 pixel area. However, if the original image or the translated version of the same image is at the edge of the $(n-10)*(n-10)$ pixel area of possible translation, as explained in the previous answer, the final pooling layer output will be different for the original image and the translated image, thus resulting in translational invariance.

    % subsection double_convolution_and_max_pooling (end)


% section object_recognition (end)

\end{document}
