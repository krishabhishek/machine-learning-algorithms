\documentclass[parskip=half]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath {{images/}}
\hypersetup{
    colorlinks=true,
    citecolor=blue
}

\renewcommand\thesubsection{\thesection.\alph{subsection}}


\begin{document}


\title{CS698 - Assignment 4}
\subtitle{Winter 2017}
\author{
    Vineet John\\
    \texttt{v2john@uwaterloo.ca}
}
\date{\today}
\maketitle


\section{Generalized and Practical Hidden Markov Model} % (fold)
\label{sec:generalized_and_practical_hidden_markov_model}


    \subsection{HMM Parametrization} % (fold)
    \label{sub:hmm_parametrization}

        \textbf{Objective:}\\
        Parametrize an HMM in such a way that that state at a given time-step $y_t$ depends on the input at the same time-step $x_t$, as well as the states of the previous two time steps $y_{t-1}, y_{t-2}$.\\

        \textbf{Proof:}
        \begin{itemize}
            \item 
            From the objective, it is evident that the aspects of the HMM model parameters that will change are the \underline{transition distribution}, and the \underline{initial state distribution}, as these are the parameters that attempt to predict a state given previous state information. The emission distribution will change only in it's generative term i.e. the class $y_t$ in $P(x_t|y_t)$
            \item 
            Consider a scenario where the set of valid states is $\{c_1, c_2\}$
            \item 
            The initial state distribution would contain:
            \begin{equation} \label{eqn:initial_state_dist_original}
                P(y_1) \in \begin{bmatrix} P(y_1 = c_1) \\ P(y_1 = c_2) \end{bmatrix}
            \end{equation}
            \item 
            Previously the transition matrix would contain the following set of transitions:
            \begin{equation} \label{eqn:transition_matrix_original}
                P(y_t|y_{t-1}) \in \begin{bmatrix} P(c_1|c_1) \\ P(c_1|c_2) \\ P(c_2|c_1) \\ P(c_2|c_2) \end{bmatrix}
            \end{equation}
            \item 
            As per the objective, the term $y_{t-1}$ must now encode 2 time-steps in the new parametrization of the HMM. This term will now encode a joint probability in itself, of the following permutations. This ensures, that the transition and initial state distributions consider an additional time-step in the past. Each of these joint probabilities can represent a new state, say $c_3 .. c_6$ 
            $$P({y\prime}_{t}) \in \begin{bmatrix} P(c_1, c_1) \\ P(c_1, c_2) \\ P(c_2, c_1) \\ P(c_2, c_2) \end{bmatrix} = \begin{bmatrix} P(c_3) \\ P(c_4) \\ P(c_5) \\ P(c_5) \end{bmatrix} $$
            \item 
            We can now substitute this new parametrization of $P(y_{t-1})$, denoted by ${y\prime}_{t-1}$ in Equation \ref{eqn:transition_matrix_original}, giving us
            \begin{equation} \label{eqn:transition_matrix_new}
                P({y\prime}_t|{y\prime}_{t-1})
                \in 
                \begin{bmatrix}
                    P(c_3|c_3), P(c_3|c_4), P(c_3|c_5), P(c_3|c_6), \\
                    P(c_4|c_3), P(c_4|c_4), P(c_4|c_5), P(c_4|c_6), \\
                    P(c_5|c_3), P(c_5|c_5), P(c_5|c_5), P(c_5|c_6), \\
                    P(c_6|c_3), P(c_6|c_4), P(c_6|c_5), P(c_6|c_6)
                \end{bmatrix}
            \end{equation}
            ${y\prime}_t$ here is the same as $y_t$ in the original equation \ref{eqn:transition_matrix_original}.
            \item 
            Similarly, the initial state distribution is now defined over 2 time-steps instead of 1. This too, can be expressed in terms of the new augmented set of states $c_3 .. c_6$
            \begin{equation} \label{eqn:initial_state_dist_new}
                P({y\prime}_1, {y\prime}_2) \in \begin{bmatrix} P(y_1 = c_1, y_2 = c_1) \\ P(y_1 = c_1, y_2 = c_2) \\ P(y_1 = c_2, y_2 = c_1) \\ P(y_1 = c_2, y_2 = c_2) \end{bmatrix} = \begin{bmatrix} P(y_1,y_2 = c_3) \\ P(y_1,y_2 = c_4) \\ P(y_1,y_2 = c_5) \\ P(y_1,y_2 = c_6) \end{bmatrix}
            \end{equation}
            \item 
            Assume a discrete set of observations $v_1$ and $v_2$. The original set of emission probabilities can be written as
            \begin{equation}
                P(x_t | y_t) 
                \in 
                \begin{bmatrix}
                    P(v_1|c_1), P(v_1|c_2), \\
                    P(v_2|c_2), P(v_2|c_2)
                \end{bmatrix}
            \end{equation}
            \item 
            The emission distribution will change in that the input observations $x_t$ will now depend on a length 2 tuple of states, as in Equation \ref{eqn:emission_dist_new}. 
            $$P(x_t | {y\prime}_t) = P(x_t, P(y_t, y_{t-1}))$$
            \begin{equation} \label{eqn:emission_dist_new}
                P(x_t | {y\prime}_t) 
                \in 
                \begin{bmatrix}
                    P(v_1|c_3), P(v_1|c_4), P(v_1|c_5), P(v_1|c_6), \\
                    P(v_2|c_3), P(v_2|c_4), P(v_2|c_5), P(v_2|c_6)
                \end{bmatrix}
            \end{equation}
        \end{itemize}

        \textbf{Conclusion:}\\
        Hence, it has been shown that the set of states can be augment to simulate joint state probabilities over 2 time-steps instead of 1, resulting in a modified parametrization of the HMM, as shown in equations \ref{eqn:transition_matrix_new}, \ref{eqn:initial_state_dist_new} and \ref{eqn:emission_dist_new}.
    
    % subsection hmm_parametrization (end)


    \subsection{Reparametrization - Number of parameters} % (fold)
    \label{sub:reparametrization_number_of_parameters}

        \textbf{Objective:}\\
        Determine and justify whether the reparametrization can be done without an increase in the number of parameters.

        \textbf{Proof:}\\
        Assuming the same example shown in Section~\ref{sub:hmm_parametrization} is applicable, the count of parameters is given in Table \ref{tab:hmm-original-vs-reparametrized}.
        \begin{table}[th]
            \centering
            \begin{tabular}{| l | c | c |}
            \hline
            \textbf{Parameter} & \textbf{Original} & \textbf{Reparametrized} \\
            \hline
            \hline
                Initial State & 2 & 4 \\
            \hline
                Transition Probabilities & 4 & 16 \\
            \hline
                Emission Probabilities & 4 & 8 \\
            \hline
            \end{tabular}
            \caption{HMM - Original vs. Reparametrized - Number of parameters}
            \label{tab:hmm-original-vs-reparametrized}
        \end{table}

        \textbf{Conclusion:}\\
        The number of parameters in the reparametrization are higher than the number of parameters in the original HMM system. This leads to the conclusion that the number of parameters cannot remain the same after generalizing an HMM to consider states prior to the immediately preceding time-step.
    
    % subsection reparametrization_number_of_parameters (end)

% section generalized_and_practical_hidden_markov_model (end)


\newpage


\section{HMM - Maximization of the likelihood of a set of sequences of hand postures} % (fold)
\label{sec:hmm_maximization_of_the_likelihood_of_a_set_of_sequences_of_hand_postures}

    

% section hmm_maximization_of_the_likelihood_of_a_set_of_sequences_of_hand_postures (end)


\newpage


\section{HMM Implementation} % (fold)
\label{sec:hmm_implementation}


    \subsection{HMM vs. GMM - Parameters} % (fold)
    \label{sub:hmm_vs_gmm_parameters}

        The code for printing the parameters for the Hidden Markov and Gaussian Mixture models is present in the folder `\texttt{hmm-algorithms}'. The README file contains instructions to run the code for GMM and HMM.

        \textbf{Gaussian Mixture Model Parameters:}
        All of these parameters were learned only from the training data and label sequence files.
        \begin{itemize}

            \item 
            Class Properties: Table \ref{tab:gmm_class_properties} contains the `\texttt{mean}' and gaussian `\texttt{prior}' for each of the classes in the training set. The keys for this dictonary are the class identifiers `1', `2' and `3'.
            \begin{table}[ht]
                \centering
                \begin{tabular}{| c | c | c |}
                \hline
                \textbf{Class} & \textbf{Mean} & \textbf{Prior Probability} \\
                \hline
                \hline
                    1 & $\begin{bmatrix}3.06309217 & 0.95674654\end{bmatrix}$ & 0.434 \\
                \hline
                    2 & $\begin{bmatrix}2.03301478 & 2.07971921\end{bmatrix}$ & 0.406 \\
                \hline
                    3 & $\begin{bmatrix}0.895225 & 2.8185375\end{bmatrix}$ & 0.16 \\
                \hline
                \end{tabular}
                \caption{GMM - Class Properties}
                \label{tab:gmm_class_properties}
            \end{table}
            \item 
            Covariance Matrix: The covariance matrix is listed below. 
            $$\Sigma = 
                \begin{bmatrix}
                    1.17971474 & 0.41991072 \\
                    0.41991072 & 1.09360673 
                \end{bmatrix}$$

        \end{itemize}
    

        \textbf{Hidden Markov Model Parameters:}
        All of these parameters were learned only from the training data and label sequence files.
        \begin{itemize}

            \item 
            Initial State Distribution: All of the train label sequences have the elements at the first time-step as `2'. It follows that the initial state distribution will be biased towards the label `2'. Table \ref{tab:hmm_initial_state_distribution} defines the initial state distribution.
            \begin{table}[ht]
                \centering
                \begin{tabular}{| c | c | c |}
                \hline
                \textbf{Class} & \textbf{Initial State Probability} \\
                \hline
                \hline
                    1 & 0 \\
                \hline
                    2 & 1 \\
                \hline
                    3 & 0 \\
                \hline
                \end{tabular}
                \caption{HMM - Initial State Distribution}
                \label{tab:hmm_initial_state_distribution}
            \end{table}

            \item 
            Multinomial Transition Distribution: Assuming each label file sequence to represent a separate experiment, the transition probabilities computed are present in
            \begin{table}[ht]
                \centering
                \begin{tabular}{| c | c | c | c |}
                \hline
                \diagbox{$class_t$}{$class_{t+1}$} & \textbf{1} & \textbf{2} & \textbf{3}\\
                \hline
                \hline
                    \textbf{1} & 0.8785 & 0.0888 & 0.0327 \\
                \hline
                    \textbf{2} & 0.1144 & 0.8408 & 0.0448 \\
                \hline
                    \textbf{3} & 0.075 & 0.125 & 0.8 \\
                \hline
                \end{tabular}
                \caption{HMM - Transition Probability Distribution}
                \label{tab:hmm_transition_probability_distribution}
            \end{table}

            \item 
            Gaussian Emission Distribution: Since the input domain is continuous, the Gaussian distribution parameters for each of the classes are given below:
                \begin{itemize}

                    \item 
                    Gaussian Emission Properties: Table \ref{tab:hmm_gaussian_emission_distribution} contains the `\texttt{mean}' for each of the classes in the training set. The keys for this dictonary are the class identifiers `1', `2' and `3'.
                    \begin{table}[ht]
                        \centering
                        \begin{tabular}{| c | c | c |}
                        \hline
                        \textbf{Class} & \textbf{Mean}\\
                        \hline
                        \hline
                            1 & $\begin{bmatrix}3.06309217 & 0.95674654\end{bmatrix}$ \\
                        \hline
                            2 & $\begin{bmatrix}2.03301478 & 2.07971921\end{bmatrix}$ \\
                        \hline
                            3 & $\begin{bmatrix}0.895225 & 2.8185375\end{bmatrix}$ \\
                        \hline
                        \end{tabular}
                        \caption{HMM - Gaussian Emission Distribution}
                        \label{tab:hmm_gaussian_emission_distribution}
                    \end{table}
                    \item 
                    Covariance Matrix: The covariance matrix is listed below. 
                    $$\Sigma = 
                        \begin{bmatrix}
                            1.17971474 & 0.41991072 \\
                            0.41991072 & 1.09360673 
                        \end{bmatrix}$$

                \end{itemize}

        \end{itemize}

    % subsection hmm_vs_gmm_parameters (end)


    \subsection{HMM Forward Algorithm vs. GMM - Accuracy} % (fold)
    \label{sub:hmm_forward_algorithm_vs_gmm_accuracy}

        The code for the Hidden Markov model Forward Algorithm and Gaussian Mixture models is present in the folder `\texttt{hmm-algorithms}'. The README file contains instructions to run the code for GMM and HMM - Forward Algorithm.

        The algorithms take into account the parameters learned from the training set (Section~\ref{sub:hmm_vs_gmm_parameters}), and uses that information to predict the test set labels.

        The monitoring accuracy is given in Table \ref{tab:hmm-forward-algorithm-vs-gmm-monitoring-accuracy}.

        \begin{table}[ht]
            \centering
            \begin{tabular}{| l | r |}
            \hline
            \textbf{Classifier} & \textbf{Monitoring Accuracy} \\
            \hline
            \hline
                Gaussian Mixture Model & 0.734 \\
            \hline
                Hidden Markov Model - Forward Algorithm & 0.804 \\
            \hline
            \end{tabular}
            \caption{HMM Forward Algorithm vs. GMM - Monitoring Accuracy}
            \label{tab:hmm-forward-algorithm-vs-gmm-monitoring-accuracy}
        \end{table}

        \subsubsection*{GMM vs. HMM - Result Discussion} % (fold)
        \label{ssub:gmm_vs_hmm_result_discussion}

            It is observed that the Hidden Markov Model classifier outperform the Gaussian Mixture model classifier in terms of the Monitoring Accuracy. This could be attributed to the fact that the Gaussian Mixture model merely uses the Gaussian probability density function to estimate the likelihood of an input data-point being a part of each class, whereas the Hidden Markov model incorporates additional training parameters that allows it to use the inital state of the experiments and transitions from previous observations. 

            These additional parameters used by the HMM encode information about the nature of the sequential data, which is bound to perform better than a naive isolated observation approach.
        
        % subsubsection gmm_vs_hmm_result_discussion (end)

    % subsection hmm_forward_algorithm_vs_gmm_accuracy (end)


    \subsection{HMM Viterbi Algorithm Accuracy} % (fold)
    \label{sub:hmm_viterbi_algorithm_accuracy}

        The code for the Hidden Markov model Viterbi Algorithm is present in the folder `\texttt{hmm-algorithms}'. The README file contains instructions to run the code.

        The classification accuracy is given in Table \ref{tab:hmm-forward-vs-hmm-viterbi-accuracy}. The accuracy for the Forward Algorithm is included for a comparison.

        \begin{table}[ht]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Classifier} & \textbf{Accuracy} & \textbf{Run Time (millis)} \\
            \hline
            \hline
                Hidden Markov Model - Forward Algorithm & 0.804 & 30.16\\
            \hline
                Hidden Markov Model - Viterbi Algorithm & 0.866 & 28.06 \\
            \hline
            \end{tabular}
            \caption{HMM Forward Algorithm vs. HMM Viterbi Algorithm - Monitoring Accuracy}
            \label{tab:hmm-forward-vs-hmm-viterbi-accuracy}
        \end{table}

        \subsubsection*{HMM - Forward Algorithm vs. Viterbi Algorithm - Result Discussion} % (fold)
        \label{ssub:hmm_forward_algo_vs_viterbi_algo_discuss}

            It is observed that the Viterbi algorithm performs better than the Forward algorithm.

            The fundamental difference between the Forward and Viterbi algorithms is that the Forward algorithm predicts only a current value given the previous state in a sequence and the set of all input observations that occurred before it, whereas the Viterbi algorithm attempts to maximize the joint probability of an entire sequence of states, given the same input.

            The dynamic programming factor which is a part of the Viterbi algorithm, allows a late binding of class prediction at each epoch, so as to take into account the states that follow it. The subsequent time-steps also influence the prediction of the Viterbi algorithm. This additional information is presumably, what makes the Viterbi algorithm more effective at predicting the probability of an entire sequence, better than the Forward algorithm, which only relies on information learned from previous epochs.
        
        % subsubsection hmm_forward_algo_vs_viterbi_algo_discuss (end)

    
    % subsection hmm_viterbi_algorithm_accuracy (end)

% section hmm_implementation (end)


\end{document}
