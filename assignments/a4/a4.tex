\documentclass[parskip=half]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath {{images/}}
\hypersetup{
    colorlinks=true,
    citecolor=blue
}

\renewcommand\thesubsection{\thesection.\alph{subsection}}
\DeclareMathOperator*{\argmax}{argmax}

\begin{document}


\title{CS698 - Assignment 4}
\subtitle{Winter 2017}
\author{
    Vineet John\\
    \texttt{v2john@uwaterloo.ca}
}
\date{\today}
\maketitle


\section{Generalized and Practical Hidden Markov Model} % (fold)
\label{sec:generalized_and_practical_hidden_markov_model}


    \subsection{HMM Parametrization} % (fold)
    \label{sub:hmm_parametrization}

        \textbf{Objective:}\\
        Parametrize an HMM in such a way that that state at a given time-step $y_t$ depends on the input at the same time-step $x_t$, as well as the states of the previous two time steps $y_{t-1}, y_{t-2}$.\\

        \textbf{Proof:}
        \begin{itemize}
            \item 
            From the objective, it is evident that the aspects of the HMM model parameters that will change are the transition distribution, emission distribution and the initial state distribution, as these are the parameters that utilize previous state information. The emission distribution will change only in it's generative term i.e. the class $y_t$ in $P(x_t|y_t)$
            \item 
            Consider a scenario where the set of valid states is $\{c_1, c_2\}$
            \item 
            In a first-order Markov model, the initial state distribution would contain:
            \begin{equation} \label{eqn:initial_state_dist_original}
                P(y_1) \in \begin{bmatrix} P(y_1 = c_1) \\ P(y_1 = c_2) \end{bmatrix}
            \end{equation}
            \item 
            In a first-order Markov model, the transition matrix would contain the following set of transitions:
            \begin{equation} \label{eqn:transition_matrix_original}
                P(y_t|y_{t-1}) \in \begin{bmatrix} P(c_1|c_1) \\ P(c_1|c_2) \\ P(c_2|c_1) \\ P(c_2|c_2) \end{bmatrix}
            \end{equation}
            \item 
            As per the objective, each output state ${y_t}\prime$ must now encode 2 time-steps in the new parametrization of the HMM. This term will now encode a joint probability in itself, of the following permutations. This ensures, that the transition and initial state distributions consider an additional time-step in the past. Each of these joint probabilities can represent a new state, say $c_3 .. c_6$ 
            $$P({y\prime}_{t}) \in \begin{bmatrix} P(c_1, c_1) \\ P(c_1, c_2) \\ P(c_2, c_1) \\ P(c_2, c_2) \end{bmatrix} = \begin{bmatrix} P(c_3) \\ P(c_4) \\ P(c_5) \\ P(c_5) \end{bmatrix} $$
            \item 
            We can now substitute this new parametrization of $P(y_{t-1})$, denoted by ${y\prime}_{t-1}$ in Equation \ref{eqn:transition_matrix_original}, giving us
            \begin{equation} \label{eqn:transition_matrix_new}
                P({y\prime}_t|{y\prime}_{t-1})
                \in 
                \begin{bmatrix}
                    P(c_3|c_3), P(c_3|c_4), P(c_3|c_5), P(c_3|c_6), \\
                    P(c_4|c_3), P(c_4|c_4), P(c_4|c_5), P(c_4|c_6), \\
                    P(c_5|c_3), P(c_5|c_5), P(c_5|c_5), P(c_5|c_6), \\
                    P(c_6|c_3), P(c_6|c_4), P(c_6|c_5), P(c_6|c_6)
                \end{bmatrix}
            \end{equation}
            ${y\prime}_t$ here is the same as $y_t$ in the original equation \ref{eqn:transition_matrix_original}.
            \item 
            Similarly, the initial state distribution is now defined over 2 time-steps instead of 1. This too, can be expressed in terms of the new augmented set of states $c_3 .. c_6$
            \begin{equation} \label{eqn:initial_state_dist_new}
                P({y\prime}_1, {y\prime}_2) \in \begin{bmatrix} P(y_1 = c_1, y_2 = c_1) \\ P(y_1 = c_1, y_2 = c_2) \\ P(y_1 = c_2, y_2 = c_1) \\ P(y_1 = c_2, y_2 = c_2) \end{bmatrix} = \begin{bmatrix} P(y_1,y_2 = c_3) \\ P(y_1,y_2 = c_4) \\ P(y_1,y_2 = c_5) \\ P(y_1,y_2 = c_6) \end{bmatrix}
            \end{equation}
            \item 
            Assume a discrete set of observations $v_1$ and $v_2$. The original set of emission probabilities can be written as
            \begin{equation}
                P(x_t | y_t) 
                \in 
                \begin{bmatrix}
                    P(v_1|c_1), P(v_1|c_2), \\
                    P(v_2|c_2), P(v_2|c_2)
                \end{bmatrix}
            \end{equation}
            \item 
            The emission distribution will change in that the input observations $x_t$ will now depend on a length 2 tuple of states, as in Equation \ref{eqn:emission_dist_new}. 
            $$P(x_t | {y\prime}_t) = P(x_t, P(y_t, y_{t-1}))$$
            \begin{equation} \label{eqn:emission_dist_new}
                P(x_t | {y\prime}_t) 
                \in 
                \begin{bmatrix}
                    P(v_1|c_3), P(v_1|c_4), P(v_1|c_5), P(v_1|c_6), \\
                    P(v_2|c_3), P(v_2|c_4), P(v_2|c_5), P(v_2|c_6)
                \end{bmatrix}
            \end{equation}
            \item 
            The first observation $x_1$ can be ignored in this formulation, so that we have $(n-1)$ states to correspond to $(n-1)$ observations, where $n$ is the number of states and observations in the original formulations of the problem.
        \end{itemize}

        \textbf{Conclusion:}\\
        Hence, it has been shown that the set of states can be augment to simulate joint state probabilities over 2 time-steps instead of 1, resulting in a modified parametrization of the HMM, as shown in equations \ref{eqn:transition_matrix_new}, \ref{eqn:initial_state_dist_new} and \ref{eqn:emission_dist_new}.
    
    % subsection hmm_parametrization (end)


    \subsection{Reparametrization - Number of parameters} % (fold)
    \label{sub:reparametrization_number_of_parameters}

        \textbf{Objective:}\\
        Determine and justify whether the reparametrization can be done without an increase in the number of parameters.

        \textbf{Proof:}\\
        Assuming the same example shown in Section~\ref{sub:hmm_parametrization} is applicable, the count of parameters is given in Table \ref{tab:hmm-original-vs-reparametrized}.
        \begin{table}[th]
            \centering
            \begin{tabular}{| l | c | c |}
            \hline
            \textbf{Parameter} & \textbf{Size (Original)} & \textbf{Size (Reparametrized)} \\
            \hline
            \hline
                Initial State & 2 & 4 \\
            \hline
                Transition Probabilities & 4 & 16 \\
            \hline
                Emission Probabilities & 4 & 8 \\
            \hline
            \end{tabular}
            \caption{HMM - Original vs. Reparametrized - Number of parameters}
            \label{tab:hmm-original-vs-reparametrized}
        \end{table}

        \textbf{Conclusion:}\\
        The number of parameters in the reparametrization are the same than the number of parameters in the original HMM system (i.e.) the initial state distribution, transition distribution and emission distribution. However, the space complexity of each parameter increases with the number of time-steps to look backwards by. 

    % subsection reparametrization_number_of_parameters (end)

% section generalized_and_practical_hidden_markov_model (end)


\newpage


\section{HMM - Maximization of the likelihood of a set of sequences of hand postures} % (fold)
\label{sec:hmm_maximization_of_the_likelihood_of_a_set_of_sequences_of_hand_postures}

    \textbf{Objective:}\\
    In some applications, HMMs are used to model a sequence of observations while the hidden states are treated as latent variables without any meaning. For instance, in gesture recognition, an HMM can be used to model a sequence of hand postures (e.g., location, position and orientation ) defining a gesture. Specify a mathematical objective to train an HMM to maximize the likelihood of a set of sequences of hand postures. More precisely, formulate an optimization problem that could be used to learn the parameters of the HMM despite the fact that the hidden states are never observed. 

    \textbf{Proof}:
    \begin{itemize}
        \item 
        The Baum-Welch algorithm can be used to obtain an objective that attempts to predict the HMM parameters in the event that the states are treated as hidden/latent variable. \footnote{Tu, Stephen. "Derivation of baum-welch algorithm for hidden markov models." (2015).}
        \item 
        The objective used in the Forward and Viterbi algorithms assumes that the set of states $Y$ is known. This objective is represented as:
        \begin{equation} \label{eqn:theta-original}
            \theta^* = \argmax_{\theta} P(X, Y, \theta)
        \end{equation}
        where $X$ is the set of observations, $Y$ is the set of states and $\theta$ represents the set of HMM parameters.
        \item 
        $\theta = (\pi, T, E)$ where $\pi$ represents the initial state distribution, $T$ represents the transition matrix and $E$ represents the emission matrix.
        \item 
        The Baum-Welch algorithm uses an iterative process, to keep re-computing $\theta$. In the original formulation where the set of output states $Y$ is known, $\theta$ can be computed using Equation \ref{eqn:theta-original}. However, in the new objective, we use
        \begin{equation*}
            \theta^* = \argmax_{\theta} \sum_{y \in Y} P(X, y, \theta)
        \end{equation*}
        \item 
        Firstly the joint probability $P(y,X)$ can be written as $P(y,X) = P(X) P(y|X)$
        $$\argmax_\theta Q(\theta, \theta^s) = \argmax_\theta \sum_{y \in Y} log [P(X,y;\theta)] P(y,X;\theta^s) $$ 
        \item 
        The following steps are repeated until convergence:
        \begin{itemize}
            \item 
            Compute 
            \begin{equation} \label{eqn:q-1}
                Q(\theta, \theta^s) = \sum_{y \in Y} log [P(X,y;\theta)] P(y,X;\theta^s)
            \end{equation}
            \item 
            Set $\theta^{s+1} = \argmax_\theta Q(\theta, \theta^s)$
        \end{itemize}
        \item 
        The transition probability is defined by:
        $$P(y_t,y_{t+1}) = \prod^N_{i,j = 1} [T_{ij}]^{y_t^iy_{t+1}^j} $$
        The initial distribution is defined by:
        $$\pi(y_0) = {\pi_i}^{q_0^i} $$
        Assuming the emission distribution is multinomial, we can define it as:
        $$P(x_t|y_t, E) = \prod^N_{i,j = 1} [E_{ij}]^{y_t^ix_t^j} $$
        \item 
        \begin{equation*}
            P(y,x; \theta) = \prod^D_{k=1} ({\pi_i}^{y_0^i} \prod^N_{i,j=1} [T_{ij}]^{y_t^iy_{t+1}^j} [E_{ij}]^{y_t^ix_t^j} )
        \end{equation*}
        where $D$ is the number of observation sequences.
        \item 
        Computing the log of this equation, we get,
        \begin{equation*}
            log(P(y,x; \theta)) = \sum^D_{k=1} log {\pi_i}^{y_0^i} + \sum^D_{k=1}\sum^N_{i,j=1} log [T_{ij}]^{y_t^iy_{t+1}^j} + \sum^D_{k=1}\sum^N_{i,j=1} log [E_{ij}]^{y_t^ix_t^j} 
        \end{equation*}
        \item 
        Now, we substitute this result for $log(P(y,x; \theta))$ in equation \ref{eqn:q-1}.
        \begin{equation*}
            Q(\theta, \theta^s) = \sum_{y \in Y} \sum^D_{k=1} log {\pi_i}^{y_0^i} + \sum_{y \in Y} \sum^D_{k=1}\sum^N_{i,j=1} [T_{ij}]^{y_t^iy_{t+1}^j} + \sum_{y \in Y} \sum^D_{k=1}\sum^N_{i,j=1} [E_{ij}]^{y_t^ix_t^j} 
        \end{equation*}
        \item 
        Each of $\pi, T, E$ are contrained by the fact that they are valid probability distributions. i.e.
        \begin{equation*}
            \sum^M_{i=1} \pi = 1
        \end{equation*}
        \begin{equation*}
            \sum^M_{j=1} T_{ij} = 1
        \end{equation*}
        \begin{equation*}
            \sum^M_{j=1} E_{ij} = 1
        \end{equation*}
        \item 
        We can now create an optimization objective with Lagrange Multiplier, such that
        \begin{equation} \label{eqn:optimization-objective}
            L(\theta, \theta^s) = Q(\theta, \theta^s) - \lambda_{\pi} (\sum^M_{i=1} \pi - 1) - \sum^M_{i=1} \lambda_{T_i} (\sum^M_{j=1} T_{ij} - 1) - \sum^M_{i=1} \lambda_{E_i} (\sum^M_{j=1} E_{ij} - 1)
        \end{equation}
        \item 
        This optimization objective can now be used to learn the HMM parameters, by setting the partial derivatives for each of the parameters $\pi, T, E$ to 0 in turn 
    \end{itemize}

    \textbf{Conclusion:}\\
    The derived optimization objective to learn the parameters of the Hidden Markov model without the knowledge of the space of output states, is given in equation \ref{eqn:optimization-objective}.


% section hmm_maximization_of_the_likelihood_of_a_set_of_sequences_of_hand_postures (end)


\newpage


\section{HMM Implementation} % (fold)
\label{sec:hmm_implementation}


    \subsection{HMM vs. GMM - Parameters} % (fold)
    \label{sub:hmm_vs_gmm_parameters}

        The code for printing the parameters for the Hidden Markov and Gaussian Mixture models is present in the folder `\texttt{hmm-algorithms}'. The README file contains instructions to run the code for GMM and HMM.

        \textbf{Gaussian Mixture Model Parameters:}
        All of these parameters were learned only from the training data and label sequence files.
        \begin{itemize}

            \item 
            Class Properties: Table \ref{tab:gmm_class_properties} contains the `\texttt{mean}' and gaussian `\texttt{prior}' for each of the classes in the training set. The keys for this dictonary are the class identifiers `1', `2' and `3'.
            \begin{table}[ht]
                \centering
                \begin{tabular}{| c | c | c |}
                \hline
                \textbf{Class} & \textbf{Mean} & \textbf{Prior Probability} \\
                \hline
                \hline
                    1 & $\begin{bmatrix}3.06309217 & 0.95674654\end{bmatrix}$ & 0.434 \\
                \hline
                    2 & $\begin{bmatrix}2.03301478 & 2.07971921\end{bmatrix}$ & 0.406 \\
                \hline
                    3 & $\begin{bmatrix}0.895225 & 2.8185375\end{bmatrix}$ & 0.16 \\
                \hline
                \end{tabular}
                \caption{GMM - Class Properties}
                \label{tab:gmm_class_properties}
            \end{table}
            \item 
            Covariance Matrix: The covariance matrix is listed below. 
            $$\Sigma = 
                \begin{bmatrix}
                    1.17971474 & 0.41991072 \\
                    0.41991072 & 1.09360673 
                \end{bmatrix}$$

        \end{itemize}
    

        \textbf{Hidden Markov Model Parameters:}
        All of these parameters were learned only from the training data and label sequence files.
        \begin{itemize}

            \item 
            Initial State Distribution: All of the train label sequences have the elements at the first time-step as `2'. It follows that the initial state distribution will be biased towards the label `2'. Table \ref{tab:hmm_initial_state_distribution} defines the initial state distribution.
            \begin{table}[ht]
                \centering
                \begin{tabular}{| c | c | c |}
                \hline
                \textbf{Class} & \textbf{Initial State Probability} \\
                \hline
                \hline
                    1 & 0 \\
                \hline
                    2 & 1 \\
                \hline
                    3 & 0 \\
                \hline
                \end{tabular}
                \caption{HMM - Initial State Distribution}
                \label{tab:hmm_initial_state_distribution}
            \end{table}

            \item 
            Multinomial Transition Distribution: Assuming each label file sequence to represent a separate experiment, the transition probabilities computed are present in Table \ref{tab:hmm_transition_probability_distribution}
            \begin{table}[ht]
                \centering
                \begin{tabular}{| c | c | c | c |}
                \hline
                \diagbox{$class_t$}{$class_{t+1}$} & \textbf{1} & \textbf{2} & \textbf{3}\\
                \hline
                \hline
                    \textbf{1} & 0.8785 & 0.0888 & 0.0327 \\
                \hline
                    \textbf{2} & 0.1144 & 0.8408 & 0.0448 \\
                \hline
                    \textbf{3} & 0.075 & 0.125 & 0.8 \\
                \hline
                \end{tabular}
                \caption{HMM - Transition Probability Distribution}
                \label{tab:hmm_transition_probability_distribution}
            \end{table}

            \item 
            Gaussian Emission Distribution: Since the input domain is continuous, the Gaussian distribution parameters for each of the classes are given below:
                \begin{itemize}

                    \item 
                    Gaussian Emission Properties: Table \ref{tab:hmm_gaussian_emission_distribution} contains the `\texttt{mean}' for each of the classes in the training set. The keys for this dictonary are the class identifiers `1', `2' and `3'.
                    \begin{table}[ht]
                        \centering
                        \begin{tabular}{| c | c | c |}
                        \hline
                        \textbf{Class} & \textbf{Mean}\\
                        \hline
                        \hline
                            1 & $\begin{bmatrix}3.06309217 & 0.95674654\end{bmatrix}$ \\
                        \hline
                            2 & $\begin{bmatrix}2.03301478 & 2.07971921\end{bmatrix}$ \\
                        \hline
                            3 & $\begin{bmatrix}0.895225 & 2.8185375\end{bmatrix}$ \\
                        \hline
                        \end{tabular}
                        \caption{HMM - Gaussian Emission Distribution}
                        \label{tab:hmm_gaussian_emission_distribution}
                    \end{table}
                    \item 
                    Covariance Matrix: The covariance matrix is listed below. 
                    $$\Sigma = 
                        \begin{bmatrix}
                            1.17971474 & 0.41991072 \\
                            0.41991072 & 1.09360673 
                        \end{bmatrix}$$

                \end{itemize}

        \end{itemize}

    % subsection hmm_vs_gmm_parameters (end)


    \subsection{HMM Forward Algorithm vs. GMM - Accuracy} % (fold)
    \label{sub:hmm_forward_algorithm_vs_gmm_accuracy}

        The code for the Hidden Markov model Forward Algorithm and Gaussian Mixture models is present in the folder `\texttt{hmm-algorithms}'. The README file contains instructions to run the code for GMM and HMM - Forward Algorithm.

        The algorithms take into account the parameters learned from the training set (Section~\ref{sub:hmm_vs_gmm_parameters}), and uses that information to predict the test set labels.

        The monitoring accuracy is given in Table \ref{tab:hmm-forward-algorithm-vs-gmm-monitoring-accuracy}.

        \begin{table}[ht]
            \centering
            \begin{tabular}{| l | r |}
            \hline
            \textbf{Classifier} & \textbf{Monitoring Accuracy} \\
            \hline
            \hline
                Gaussian Mixture Model & 73.4\% \\
            \hline
                Hidden Markov Model - Forward Algorithm & 80.4\% \\
            \hline
            \end{tabular}
            \caption{HMM Forward Algorithm vs. GMM - Monitoring Accuracy}
            \label{tab:hmm-forward-algorithm-vs-gmm-monitoring-accuracy}
        \end{table}

        \subsubsection*{GMM vs. HMM - Result Discussion} % (fold)
        \label{ssub:gmm_vs_hmm_result_discussion}

            It is observed that the Hidden Markov Model classifier outperform the Gaussian Mixture model classifier in terms of the Monitoring Accuracy. This could be attributed to the fact that the Gaussian Mixture model merely uses the Gaussian probability density function to estimate the likelihood of an input data-point being a part of each class, whereas the Hidden Markov model incorporates additional training parameters that allows it to use the inital state of the experiments and transitions from previous observations. 

            These additional parameters used by the HMM encode information about the nature of the sequential data, which is bound to perform better than a naive isolated observation approach.
        
        % subsubsection gmm_vs_hmm_result_discussion (end)

    % subsection hmm_forward_algorithm_vs_gmm_accuracy (end)


    \subsection{HMM Viterbi Algorithm Accuracy} % (fold)
    \label{sub:hmm_viterbi_algorithm_accuracy}

        The code for the Hidden Markov model Viterbi Algorithm is present in the folder `\texttt{hmm-algorithms}'. The README file contains instructions to run the code.

        The classification accuracy is given in Table \ref{tab:hmm-forward-vs-hmm-viterbi-accuracy}. The accuracy for the Forward Algorithm is included for a comparison.

        \begin{table}[ht]
            \centering
            \begin{tabular}{| l | r | r |}
            \hline
            \textbf{Classifier} & \textbf{Accuracy} & \textbf{Run Time (millis)} \\
            \hline
            \hline
                Hidden Markov Model - Forward Algorithm & 80.4\% & 30.16\\
            \hline
                Hidden Markov Model - Viterbi Algorithm & 86.6\% & 28.06 \\
            \hline
            \end{tabular}
            \caption{HMM Forward Algorithm vs. HMM Viterbi Algorithm - Monitoring Accuracy}
            \label{tab:hmm-forward-vs-hmm-viterbi-accuracy}
        \end{table}

        \subsubsection*{HMM - Forward Algorithm vs. Viterbi Algorithm - Result Discussion} % (fold)
        \label{ssub:hmm_forward_algo_vs_viterbi_algo_discuss}

            It is observed that the Viterbi algorithm performs better than the Forward algorithm.

            The fundamental difference between the Forward and Viterbi algorithms is that the Forward algorithm predicts only a current value given the previous state in a sequence and the set of all input observations that occurred before it, whereas the Viterbi algorithm attempts to maximize the joint probability of an entire sequence of states, given the same input.

            The dynamic programming factor which is a part of the Viterbi algorithm, allows a late binding of class prediction at each epoch, so as to take into account the states that follow it. The subsequent time-steps also influence the prediction of the Viterbi algorithm. This additional information is presumably, what makes the Viterbi algorithm more effective at predicting the probability of an entire sequence, better than the Forward algorithm, which only relies on information learned from previous epochs.
        
        % subsubsection hmm_forward_algo_vs_viterbi_algo_discuss (end)

    
    % subsection hmm_viterbi_algorithm_accuracy (end)

% section hmm_implementation (end)


\end{document}
