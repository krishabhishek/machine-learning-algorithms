\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{mathtools}

\title{CS698 - Winter 2017\\Assignment 1 - Q3a}
\author{Vineet John (v2john@uwaterloo.ca)}
\date{}
\begin{document}
\maketitle

\section{Problem Statement}

Theory. In class, we discussed several loss functions for linear regression. However all the loss functions that we discussed assume that the error contributed by each data point have the same importance. Consider a scenario where we would like to give more weight to some data points. Our goal is to fit the data points $(x_n , y_n )$ in proportion to their weights $r_n$ by minimizing the following objective:

$$L(w, b) = \sum_{n=1}^m r_n (y_n − wx_n + b)^2$$

\section{Solution}

\subsection{Calculation of the partial derivatives}

Since the loss function needs to be minimized with respect to 2 variables, the partial derivative of the loss function L with respect to each variable is calculated independently.

\begin{align*}
	\frac{\partial L}{\partial w} &= 0\\
	2 * \sum_{n=1}^m r_n (y_n − wx_n + b) * (x_n) &= 0\\
	\sum_{n=1}^m r_n (y_n − wx_n + b) * (x_n) &= 0\\
	\sum_{n=1}^m r_n (y_nx_n − wx_nx_n^T + bx_n) &= 0\\
	\sum_{n=1}^m r_n y_n x_n − r_n w x_n x_n^T + r_n b x_n &= 0\\
	\sum_{n=1}^m r_n y_n x_n − w \sum_{n=1}^m r_n x_n x_n^T + b \sum_{n=1}^m r_n x_n &= 0\\
	w \sum_{n=1}^m r_n x_n x_n^T &=  \sum_{n=1}^m r_n y_n x_n + b \sum_{n=1}^m r_n x_n
\end{align*}

\begin{align}
\label{pd_w}
	\Aboxed{w  = \frac{b \sum_{n=1}^m r_n x_n + \sum_{n=1}^m r_n y_n x_n} {\sum_{n=1}^m r_n x_n x_n^T}}
\end{align}

\vspace{5mm}

Also, 

\begin{align*}
	\frac{\partial L}{\partial b} &= 0\\
	2 * \sum_{n=1}^m r_n (y_n − w x_n + b) &= 0\\
	\sum_{n=1}^m r_n (y_n − w x_n + b) &= 0\\
	\sum_{n=1}^m r_n (y_n − w x_n + b) &= 0\\
	\sum_{n=1}^m r_n y_n − r_n w x_n + r_n b &= 0\\
	\sum_{n=1}^m r_n y_n − w \sum_{n=1}^m r_n x_n + b \sum_{n=1}^m r_n &= 0\\
	b \sum_{n=1}^m r_n &= w \sum_{n=1}^m r_n x_n - \sum_{n=1}^m r_n y_n
\end{align*}

\begin{align}
\label{pd_b}
	\Aboxed{b = \frac{w \sum_{n=1}^m r_n x_n - \sum_{n=1}^m  r_n y_n}{\sum_{n=1}^m  r_n}}
\end{align}


\subsection{Solution of the system of linear equations}

The value of b, as obtained in Equation \ref{pd_b} is substituted in Equation \ref{pd_w}.

\begin{align*}
	w &= \frac{\frac{w \sum_{n=1}^m r_n x_n - \sum_{n=1}^m r_n y_n}{\sum_{n=1}^m r_n} \sum_{n=1}^m r_n x_n + \sum_{n=1}^m r_n y_n x_n} {\sum_{n=1}^m r_n x_n x_n^T}\\
	w &= \frac{(w \sum_{n=1}^m r_n x_n - \sum_{n=1}^m r_n y_n) \sum_{n=1}^m r_n x_n + \sum_{n=1}^m r_n y_n x_n \sum_{n=1}^m r_n} {\sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m r_n}\\
	w &= \frac{w ({\sum_{n=1}^m r_n x_n})^2 - \sum_{n=1}^m r_n y_n \sum_{n=1}^m r_n x_n + \sum_{n=1}^m r_n y_n x_n \sum_{n=1}^m r_n} {\sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m r_n}\\
	w \sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m  r_n &= w ({\sum_{n=1}^m r_n x_n})^2 - \sum_{n=1}^m  r_n y_n \sum_{n=1}^m  r_n x_n + \sum_{n=1}^m  r_n y_n x_n \sum_{n=1}^m  r_n\\
	w \sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m  r_n - w ({\sum_{n=1}^m r_n x_n})^2 &= - \sum_{n=1}^m  r_n y_n \sum_{n=1}^m  r_n x_n + \sum_{n=1}^m  r_n y_n x_n \sum_{n=1}^m  r_n\\
	w (\sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m  r_n - ({\sum_{n=1}^m r_n x_n})^2) &= - \sum_{n=1}^m  r_n y_n \sum_{n=1}^m  r_n x_n + \sum_{n=1}^m  r_n y_n x_n \sum_{n=1}^m  r_n\\
\end{align*}

\begin{align}
\label{cf_w}
	\Aboxed{w  = \frac{\sum_{n=1}^m  r_n y_n x_n \sum_{n=1}^m  r_n - \sum_{n=1}^m  r_n y_n \sum_{n=1}^m  r_n x_n}{\sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m  r_n - ({\sum_{n=1}^m r_n x_n})^2}}
\end{align}

\vspace{10mm}

Now that the value of $w$ has been expressed in terms of $r_n$, $x_n$ and $y_n$, we can substitute this value of w in Equation \ref{pd_b} to obtain a closed form expression for $b$.

\begin{align*}
	b &= \frac{w \sum_{n=1}^m r_n x_n - \sum_{n=1}^m  r_n y_n}{\sum_{n=1}^m  r_n} \\
	b \sum_{n=1}^m  r_n &= w \sum_{n=1}^m r_n x_n - \sum_{n=1}^m  r_n y_n\\
	b \sum_{n=1}^m  r_n &= \frac{\sum_{n=1}^m  r_n y_n x_n \sum_{n=1}^m  r_n - \sum_{n=1}^m  r_n y_n \sum_{n=1}^m  r_n x_n}{\sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m  r_n - ({\sum_{n=1}^m r_n x_n})^2} \sum_{n=1}^m r_n x_n - \sum_{n=1}^m  r_n y_n\\
	b \sum_{n=1}^m  r_n &= \frac{\sum_{n=1}^m  r_n y_n x_n \sum_{n=1}^m  r_n - \sum_{n=1}^m  r_n y_n \sum_{n=1}^m  r_n x_n}{\sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m  r_n - ({\sum_{n=1}^m r_n x_n})^2} \sum_{n=1}^m r_n x_n - \sum_{n=1}^m  r_n y_n\\
	b \sum_{n=1}^m  r_n &= \frac{\sum_{n=1}^m  r_n y_n x_n \sum_{n=1}^m  r_n \sum_{n=1}^m r_n x_n - \sum_{n=1}^m  r_n y_n (\sum_{n=1}^m  r_n x_n)^2}{\sum_{n=1}^m r_n x_n x_n^T \sum_{n=1}^m  r_n - ({\sum_{n=1}^m r_n x_n})^2} - \sum_{n=1}^m  r_n y_n
\end{align*}

\begin{align}
\label{cf_b}
	\Aboxed{b &= \frac{\sum_{n=1}^m r_n y_n x_n \sum_{n=1}^m r_n \sum_{n=1}^mr_n x_n - \sum_{n=1}^m r_n y_n (\sum_{n=1}^m r_n x_n)^2}{\sum_{n=1}^mr_n x_n x_n^T (\sum_{n=1}^m r_n)^2 - \sum_{n=1}^m r_n ({\sum_{n=1}^mr_n x_n})^2} - \frac{\sum_{n=1}^m r_n y_n}{\sum_{n=1}^m r_n}}
\end{align}

\vspace{5mm}

\section{Conclusion}
Equations \ref{cf_w} and \ref{cf_b} are the closed form solutions for minimizing the loss function L, in terms of the input variable x, the target variable w, and the local weight r.

\end{document}
